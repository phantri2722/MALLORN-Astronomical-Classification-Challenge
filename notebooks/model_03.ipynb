{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class LightCurveProcessor:\n",
    "    \"\"\"\n",
    "    Lớp chịu trách nhiệm tải, làm sạch và hiệu chỉnh vật lý cho dữ liệu lightcurve LSST.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hệ số tắt dần Schlafly & Finkbeiner (2011) cho LSST filters (Rv=3.1)\n",
    "    EXTINCTION_COEFFS = {\n",
    "        'u': 4.239, 'g': 3.303, 'r': 2.285, \n",
    "        'i': 1.698, 'z': 1.263, 'y': 1.086\n",
    "    }\n",
    "\n",
    "    def __init__(self, metadata_path):\n",
    "        \"\"\"\n",
    "        Khởi tạo với đường dẫn đến file metadata chứa thông tin E(B-V).\n",
    "        \"\"\"\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        # Tạo index bằng object_id để tra cứu nhanh khi merge\n",
    "        if 'object_id' in self.metadata.columns:\n",
    "            self.metadata.set_index('object_id', inplace=True)\n",
    "        \n",
    "    def correct_extinction(self, df):\n",
    "        \"\"\"\n",
    "        Áp dụng hiệu chỉnh tắt dần thiên hà cho thông lượng.\n",
    "        Công thức: F_corr = F_obs * 10^(0.4 * A_lambda)\n",
    "        với A_lambda = R_lambda * E(B-V)\n",
    "        \"\"\"\n",
    "        # 1. Join với metadata để lấy EBV\n",
    "        if 'EBV' not in df.columns:\n",
    "            # Sử dụng map hoặc merge. Vì metadata đã set index là object_id,\n",
    "            # ta dùng merge với right_index=True để tối ưu tốc độ.\n",
    "            # Chỉ lấy cột EBV để tránh trùng lặp dữ liệu không cần thiết\n",
    "            df = df.merge(\n",
    "                self.metadata[['EBV']], \n",
    "                left_on='object_id', \n",
    "                right_index=True, \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "        # 2. Ánh xạ hệ số R_lambda dựa trên bộ lọc (SỬA LỖI DÒNG 39)\n",
    "        # Lưu vào cột mới thay vì đè lên df\n",
    "        df['R_lambda'] = df['Filter'].map(self.EXTINCTION_COEFFS)\n",
    "        \n",
    "        # 3. Tính toán độ tắt dần A_lambda (SỬA LỖI DÒNG 42)\n",
    "        # A_lambda = R_lambda * E(B-V)\n",
    "        df['A_lambda'] = df['R_lambda'] * df['EBV']\n",
    "        \n",
    "        # 4. Hệ số hiệu chỉnh thông lượng\n",
    "        correction_factor = np.power(10, 0.4 * df['A_lambda'])\n",
    "        \n",
    "        # 5. Áp dụng hiệu chỉnh cho cả Flux và Flux_err\n",
    "        df['Flux_corr'] = df['Flux'] * correction_factor\n",
    "        df['Flux_err_corr'] = df['Flux_err'] * correction_factor\n",
    "        \n",
    "        # 6. Làm sạch các cột tạm thời (SỬA LỖI DÒNG 51)\n",
    "        # Xử lý trường hợp EBV bị NaN (do không tìm thấy object_id)\n",
    "        df.dropna(subset=['Flux_corr'], inplace=True)\n",
    "        \n",
    "        return df.drop(columns=['R_lambda', 'A_lambda', 'EBV'])\n",
    "\n",
    "    def load_and_process(self, lc_path):\n",
    "        \"\"\"\n",
    "        Pipeline tải và xử lý một file csv lightcurve.\n",
    "        \"\"\"\n",
    "        print(f\"Đang tải dữ liệu từ {lc_path}...\")\n",
    "        raw_df = pd.read_csv(lc_path)\n",
    "        \n",
    "        # Pipeline xử lý\n",
    "        try:\n",
    "            processed_df = self.correct_extinction(raw_df)\n",
    "            print(\"Hoàn tất hiệu chỉnh extinction.\")\n",
    "            return processed_df\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi trong quá trình xử lý: {e}\")\n",
    "            return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel\n",
    "import numpy as np\n",
    "\n",
    "class GPRInterpolator:\n",
    "    \"\"\"\n",
    "    Thực hiện hồi quy GP để nội suy và tăng cường dữ liệu lightcurve.\n",
    "    Sử dụng Kernel Matern 3/2 phù hợp cho vật lý thiên văn.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_grid_length=100, time_range=(-50, 150)):\n",
    "        # Định nghĩa lưới thời gian chuẩn hóa: từ -50 đến +150 ngày so với đỉnh\n",
    "        self.time_grid = np.linspace(time_range, time_range, time_grid_length)\n",
    "        \n",
    "        # Cấu hình Kernel:\n",
    "        # ConstantKernel: Điều chỉnh biên độ tổng thể\n",
    "        # Matern(nu=1.5): Điều chỉnh độ trơn (smoothness)\n",
    "        # length_scale_bounds: Giới hạn thang thời gian biến đổi (1 ngày đến 100 ngày)\n",
    "        self.kernel = ConstantKernel(1.0, (1e-3, 1e3)) * \\\n",
    "                      Matern(length_scale=20, length_scale_bounds=(1, 200), nu=1.5)\n",
    "\n",
    "    def fit_predict(self, times, fluxes, errors, augment=0):\n",
    "        \"\"\"\n",
    "        Khớp mô hình GP và trả về lưới nội suy.\n",
    "        \n",
    "        Args:\n",
    "            times (array): Thời gian quan sát (đã chuẩn hóa t_peak=0).\n",
    "            fluxes (array): Thông lượng đã hiệu chỉnh.\n",
    "            errors (array): Sai số thông lượng đã hiệu chỉnh.\n",
    "            augment (int): Số lượng mẫu tăng cường cần tạo ra từ phân phối hậu nghiệm.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Ma trận shape (1 + augment, time_grid_length)\n",
    "        \"\"\"\n",
    "        # Sklearn yêu cầu input shape (N, 1)\n",
    "        X = times.reshape(-1, 1)\n",
    "        y = fluxes\n",
    "        \n",
    "        # alpha nhận giá trị phương sai (error^2) để xử lý nhiễu dị phương sai\n",
    "        alpha = errors ** 2\n",
    "        \n",
    "        # Khởi tạo GPR với n_restarts_optimizer để tránh tối ưu cục bộ\n",
    "        gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=alpha, \n",
    "                                       n_restarts_optimizer=3, normalize_y=True)\n",
    "        \n",
    "        try:\n",
    "            gpr.fit(X, y)\n",
    "        except Exception as e:\n",
    "            # Fallback an toàn nếu GPR thất bại (hiếm gặp)\n",
    "            print(f\"GPR fit failed: {e}\")\n",
    "            return np.zeros((augment + 1, len(self.time_grid)))\n",
    "\n",
    "        X_pred = self.time_grid.reshape(-1, 1)\n",
    "        \n",
    "        # 1. Dự đoán giá trị trung bình (Mean prediction) - Đây là mẫu chính\n",
    "        mu = gpr.predict(X_pred)\n",
    "        results = [mu]\n",
    "        \n",
    "        # 2. Tăng cường dữ liệu (Augmentation): Sample từ hậu nghiệm\n",
    "        if augment > 0:\n",
    "            # sample_y trả về shape (n_targets, n_samples), cần transpose\n",
    "            samples = gpr.sample_y(X_pred, n_samples=augment, random_state=42)\n",
    "            for i in range(augment):\n",
    "                results.append(samples[:, i])\n",
    "                \n",
    "        return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import skew, kurtosis, linregress\n",
    "\n",
    "class PhysicsFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.bands = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "        self.n_feat_per_band = 26\n",
    "        self.n_color_feat = 10\n",
    "        self.total_features = (6 * 26) + 10 # = 166\n",
    "\n",
    "    @staticmethod\n",
    "    def bazin(t, A, B, t0, t_fall, t_rise):\n",
    "        with np.errstate(over='ignore', invalid='ignore'):\n",
    "            exp_fall = np.exp(-(t - t0) / np.clip(t_fall, 1e-3, 500))\n",
    "            exp_rise = np.exp(-(t - t0) / np.clip(t_rise, 1e-3, 200))\n",
    "            val = A * (exp_fall / (1 + exp_rise)) + B\n",
    "        return np.nan_to_num(val)\n",
    "\n",
    "    def fit_bazin(self, times, fluxes):\n",
    "        if len(fluxes) <= 5: return [0]*6 # Fix: Phải > 5 mới tính được chi2\n",
    "        try:\n",
    "            p0 = [np.max(fluxes)-np.min(fluxes), np.min(fluxes), times[np.argmax(fluxes)], 30, 15]\n",
    "            bounds = ([0, -np.inf, times.min()-20, 0.1, 0.1], [np.inf, np.inf, times.max()+20, 500, 200])\n",
    "            popt, _ = curve_fit(self.bazin, times, fluxes, p0=p0, bounds=bounds, maxfev=800)\n",
    "            residuals = fluxes - self.bazin(times, *popt)\n",
    "            # Fix: Tránh chia cho 0 hoặc số âm\n",
    "            chi2 = np.sum(residuals**2) / (len(fluxes) - 5)\n",
    "            return list(popt) + [chi2]\n",
    "        except:\n",
    "            return [0]*6\n",
    "\n",
    "    def calculate_stats(self, times, fluxes, errors):\n",
    "        if len(fluxes) == 0: return [0] * 20\n",
    "        \n",
    "        # Tránh std = 0\n",
    "        f_std = np.std(fluxes)\n",
    "        f_mean = np.mean(fluxes)\n",
    "        \n",
    "        # Các chỉ số cơ bản\n",
    "        res = [\n",
    "            np.max(fluxes), np.min(fluxes), f_mean, f_std,\n",
    "            skew(fluxes) if len(fluxes) > 2 else 0,\n",
    "            kurtosis(fluxes) if len(fluxes) > 2 else 0,\n",
    "            (np.max(fluxes) - np.min(fluxes)) / 2,\n",
    "            np.median(fluxes)\n",
    "        ]\n",
    "        \n",
    "        # Percentiles\n",
    "        q5, q25, q75, q95 = np.percentile(fluxes, [5, 25, 75, 95])\n",
    "        res.extend([q5, q25, q75, q95, q75 - q25])\n",
    "        \n",
    "        # Ratios (Dùng np.divide để an toàn)\n",
    "        res.append(np.mean(fluxes**2))\n",
    "        res.append(np.mean(np.divide(fluxes, errors + 1e-6)))\n",
    "        res.append(np.sum(fluxes > f_mean) / len(fluxes))\n",
    "        \n",
    "        # Stetson K\n",
    "        delta = np.divide(fluxes - f_mean, errors + 1e-9)\n",
    "        res.append(np.sum(np.abs(delta)) / len(fluxes) * np.sqrt(1.0/len(fluxes)))\n",
    "        \n",
    "        # Slope\n",
    "        try:\n",
    "            slope, intercept, _, _, _ = linregress(times, fluxes)\n",
    "            res.extend([slope, intercept])\n",
    "        except:\n",
    "            res.extend([0, 0])\n",
    "            \n",
    "        res.append(len(fluxes))\n",
    "        return res\n",
    "\n",
    "    def extract_features(self, df_object):\n",
    "        features = []\n",
    "        band_max_flux = {b: 0 for b in self.bands}\n",
    "        band_max_time = {b: 0 for b in self.bands}\n",
    "\n",
    "        for band in self.bands:\n",
    "            band_data = df_object[df_object['Filter'] == band]\n",
    "            if len(band_data) < 3:\n",
    "                features.extend([0] * self.n_feat_per_band)\n",
    "            else:\n",
    "                times, fluxes, errors = band_data['Time'].values, band_data['Flux_corr'].values, band_data['Flux_err_corr'].values\n",
    "                stats = self.calculate_stats(times, fluxes, errors)\n",
    "                bazin = self.fit_bazin(times, fluxes) if band in ['g', 'r', 'i'] else [0]*6\n",
    "                features.extend(stats + bazin)\n",
    "                band_max_flux[band] = stats[0]\n",
    "                band_max_time[band] = times[np.argmax(fluxes)]\n",
    "\n",
    "        # Color features\n",
    "        for b1, b2 in [('u', 'g'), ('g', 'r'), ('r', 'i'), ('i', 'z'), ('z', 'y')]:\n",
    "            if band_max_flux[b1] > 0 and band_max_flux[b2] > 0:\n",
    "                color = -2.5 * np.log10(band_max_flux[b1] / band_max_flux[b2])\n",
    "                lag = band_max_time[b1] - band_max_time[b2]\n",
    "            else:\n",
    "                color, lag = 0, 0\n",
    "            features.extend([color, lag])\n",
    "            \n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Conv1DAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Mạng nơ-ron tích chập 1 chiều Autoencoder để trích xuất đặc trưng hình thái học.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_len=100, num_channels=6, latent_dim=32):\n",
    "        super(Conv1DAutoencoder, self).__init__()\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        # Input: (Batch, 6, 100)\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Layer 1: 100 -> 50\n",
    "            nn.Conv1d(num_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            # Layer 2: 50 -> 25\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            # Layer 3: 25 -> 5\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5) \n",
    "        )\n",
    "        \n",
    "        # Flatten: 64 channels * 5 length = 320 features\n",
    "        self.flatten_dim = 64 * 5\n",
    "        \n",
    "        # Latent Vector (Bottleneck)\n",
    "        self.fc_enc = nn.Linear(self.flatten_dim, latent_dim)\n",
    "        \n",
    "        # --- Decoder ---\n",
    "        self.fc_dec = nn.Linear(latent_dim, self.flatten_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # Unflatten -> (Batch, 64, 5)\n",
    "            # Layer 1: 5 -> 25\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=5, stride=5),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 2: 25 -> 50\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 3: 50 -> 100\n",
    "            nn.ConvTranspose1d(16, num_channels, kernel_size=2, stride=2),\n",
    "            # Output không dùng hàm kích hoạt để hồi quy giá trị thực\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        latent = self.fc_enc(x)\n",
    "        \n",
    "        # Decode\n",
    "        x = self.fc_dec(latent)\n",
    "        x = x.view(x.size(0), 64, 5) # Reshape\n",
    "        reconstruction = self.decoder(x)\n",
    "        \n",
    "        return reconstruction, latent\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        \"\"\"Trích xuất véc-tơ tiềm ẩn cho mục đích phân loại.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            latent = self.fc_enc(x)\n",
    "        return latent.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridEnsemble:\n",
    "    \"\"\"\n",
    "    Mô hình Stacking kết hợp XGBoost, LightGBM, CatBoost.\n",
    "    Meta-learner: Logistic Regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_pos_weight=1.0):\n",
    "        # Cấu hình các base models với tham số xử lý mất cân bằng\n",
    "        self.xgb = XGBClassifier(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            eval_metric='logloss', use_label_encoder=False\n",
    "        )\n",
    "        self.lgbm = LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            verbose=-1\n",
    "        )\n",
    "        self.cat = CatBoostClassifier(\n",
    "            iterations=500, learning_rate=0.05, \n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            verbose=0, allow_writing_files=False\n",
    "        )\n",
    "        \n",
    "        self.meta = LogisticRegression()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X_phys, X_dl, y):\n",
    "        \"\"\"\n",
    "        Huấn luyện Stacking theo quy trình Cross-Validation Out-of-Fold.\n",
    "        \"\"\"\n",
    "        # 1. Kết hợp đặc trưng và chuẩn hóa\n",
    "        X_combined = np.hstack([X_phys, X_dl])\n",
    "        X_scaled = self.scaler.fit_transform(X_combined)\n",
    "        \n",
    "        # 2. Tạo Out-of-Fold Predictions cho Meta-learner\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        meta_features = np.zeros((len(y), 3)) # 3 models\n",
    "        \n",
    "        print(\"Đang huấn luyện Base Learners (Level 0)...\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y)):\n",
    "            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Fit từng model\n",
    "            self.xgb.fit(X_train, y_train)\n",
    "            self.lgbm.fit(X_train, y_train)\n",
    "            self.cat.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict proba cho tập val\n",
    "            meta_features[val_idx, 0] = self.xgb.predict_proba(X_val)[:, 1]\n",
    "            meta_features[val_idx, 1] = self.lgbm.predict_proba(X_val)[:, 1]\n",
    "            meta_features[val_idx, 2] = self.cat.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "        # 3. Retrain base models trên toàn bộ dữ liệu\n",
    "        print(\"Retraining Base Learners trên toàn bộ dữ liệu...\")\n",
    "        self.xgb.fit(X_scaled, y)\n",
    "        self.lgbm.fit(X_scaled, y)\n",
    "        self.cat.fit(X_scaled, y)\n",
    "        \n",
    "        # 4. Train Meta-learner trên meta_features\n",
    "        print(\"Huấn luyện Meta Learner (Level 1)...\")\n",
    "        self.meta.fit(meta_features, y)\n",
    "\n",
    "    def predict_proba(self, X_phys, X_dl):\n",
    "        X_combined = np.hstack([X_phys, X_dl])\n",
    "        X_scaled = self.scaler.transform(X_combined)\n",
    "        \n",
    "        # Lấy dự đoán từ base models\n",
    "        p1 = self.xgb.predict_proba(X_scaled)[:, 1]\n",
    "        p2 = self.lgbm.predict_proba(X_scaled)[:, 1]\n",
    "        p3 = self.cat.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Meta features cho test set\n",
    "        meta_features = np.column_stack([p1, p2, p3])\n",
    "        \n",
    "        # Dự đoán cuối cùng\n",
    "        return self.meta.predict_proba(meta_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13292ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# --- HÀM XỬ LÝ 1 OBJECT (ĐÃ FIX CHẶN WARNING TẠI LUỒNG CON) ---\n",
    "def process_single_object(obj_id, obj_data, target, n_augment, phys_extractor):\n",
    "    import warnings\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning) # Chặn warning chia 0\n",
    "    \n",
    "    try:\n",
    "        if obj_data.empty: return None\n",
    "        \n",
    "        # 1. Trích xuất đặc trưng vật lý (166 đặc trưng)\n",
    "        try:\n",
    "            phys_feats = phys_extractor.extract_features(obj_data)\n",
    "            if len(phys_feats) != 166: # Kiểm tra chéo độ dài\n",
    "                phys_feats = np.zeros(166)\n",
    "        except:\n",
    "            phys_feats = np.zeros(166)\n",
    "\n",
    "        # 2. Xử lý Gaussian Process (DL Features)\n",
    "        t_peak_global = obj_data.loc[obj_data['Flux_corr'].idxmax(), 'Time']\n",
    "        time_grid = np.linspace(-50, 150, 100)\n",
    "        obj_grids = []\n",
    "        expected_shape = (n_augment + 1, 100)\n",
    "        \n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=20, nu=1.5)\n",
    "\n",
    "        for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "            band_dat = obj_data[obj_data['Filter'] == band]\n",
    "            if len(band_dat) < 3:\n",
    "                interp = np.zeros(expected_shape)\n",
    "            else:\n",
    "                X = (band_dat['Time'].values - t_peak_global).reshape(-1, 1)\n",
    "                y = band_dat['Flux_corr'].values\n",
    "                alpha = band_dat['Flux_err_corr'].values**2 + 1e-4\n",
    "                gpr = GaussianProcessRegressor(kernel=kernel, alpha=alpha, normalize_y=True)\n",
    "                try:\n",
    "                    gpr.fit(X, y)\n",
    "                    mu = gpr.predict(time_grid.reshape(-1, 1))\n",
    "                    results = [mu]\n",
    "                    if n_augment > 0:\n",
    "                        samples = gpr.sample_y(time_grid.reshape(-1, 1), n_samples=n_augment)\n",
    "                        for k in range(n_augment): results.append(samples[:, k])\n",
    "                    interp = np.array(results)\n",
    "                except:\n",
    "                    interp = np.zeros(expected_shape)\n",
    "            \n",
    "            # Chuẩn hóa shape (1 + n_aug, 100)\n",
    "            if interp.shape != expected_shape:\n",
    "                interp = np.zeros(expected_shape)\n",
    "            obj_grids.append(interp)\n",
    "\n",
    "        # Gộp các băng: (1 + n_aug, 6, 100)\n",
    "        stacked_grids = np.stack(obj_grids, axis=1)\n",
    "\n",
    "        final_res = []\n",
    "        for i in range(n_augment + 1):\n",
    "            final_res.append({\n",
    "                'dl_feat': stacked_grids[i], \n",
    "                'phys_feat': phys_feats, \n",
    "                'label': target, \n",
    "                'obj_id': obj_id\n",
    "            })\n",
    "        return final_res\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    print(\"=== BẮT ĐẦU PIPELINE TỐI ƯU (SIÊU IM LẶNG) ===\")\n",
    "    \n",
    "    BASE_DIR = '/kaggle/input/data-for-black-hole'\n",
    "    TRAIN_META_PATH = os.path.join(BASE_DIR, 'train_log.csv')\n",
    "    TEST_META_PATH = os.path.join(BASE_DIR, 'test_log.csv')\n",
    "    \n",
    "    if not os.path.exists(TRAIN_META_PATH): raise FileNotFoundError(\"Thiếu metadata!\")\n",
    "    \n",
    "    split_dirs = sorted([d for d in os.listdir(BASE_DIR) if d.startswith('split_')])\n",
    "    print(f\"-> Tìm thấy {len(split_dirs)} splits.\")\n",
    "\n",
    "    train_processor = LightCurveProcessor(TRAIN_META_PATH)\n",
    "    test_processor = LightCurveProcessor(TEST_META_PATH) if os.path.exists(TEST_META_PATH) else None\n",
    "    train_meta = pd.read_csv(TRAIN_META_PATH)\n",
    "    physics_extractor = PhysicsFeatureExtractor()\n",
    "    \n",
    "    FINAL_X_DL, FINAL_X_PHYS, FINAL_Y = [], [], []\n",
    "\n",
    "    # PHẦN 1: TRAIN\n",
    "    print(f\"\\n[PHẦN 1] Xử lý dữ liệu TRAIN (Dùng 100% CPU)...\")\n",
    "    for split_name in split_dirs:\n",
    "        LC_PATH = os.path.join(BASE_DIR, split_name, 'train_full_lightcurves.csv')\n",
    "        if not os.path.exists(LC_PATH): continue\n",
    "        \n",
    "        print(f\"  >> Đang chạy Split: {split_name}...\")\n",
    "        train_lc = train_processor.load_and_process(LC_PATH)\n",
    "        \n",
    "        # Đồng bộ cột Time\n",
    "        cols = train_lc.columns.tolist()\n",
    "        time_col = next((c for c in cols if 'mjd' in c.lower() or 'time' in c.lower()), 'Time')\n",
    "        train_lc.rename(columns={time_col: 'Time'}, inplace=True)\n",
    "        \n",
    "        unique_objs = train_lc['object_id'].unique()\n",
    "        parallel_inputs = []\n",
    "        for obj_id in unique_objs:\n",
    "            target = train_meta[train_meta['object_id'] == obj_id]['target'].values\n",
    "            if len(target) > 0:\n",
    "                obj_data = train_lc[train_lc['object_id'] == obj_id].copy()\n",
    "                parallel_inputs.append((obj_id, obj_data, target[0], 5 if target[0] == 1 else 0))\n",
    "        \n",
    "        # THỰC THI SONG SONG (Dòng quan trọng)\n",
    "        # N_JOBS=-1: Tận dụng mọi nhân CPU\n",
    "        batch_results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_single_object)(pid, pdata, ptarget, paug, physics_extractor) \n",
    "            for pid, pdata, ptarget, paug in parallel_inputs\n",
    "        )\n",
    "        \n",
    "        for res in batch_results:\n",
    "            if res:\n",
    "                for s in res:\n",
    "                    FINAL_X_DL.append(s['dl_feat'])\n",
    "                    FINAL_X_PHYS.append(s['phys_feat'])\n",
    "                    FINAL_Y.append(s['label'])\n",
    "        \n",
    "        del train_lc, batch_results; gc.collect()\n",
    "\n",
    "    # PHẦN 2 & 3: HUẤN LUYỆN (Giữ nguyên logic cũ)\n",
    "    X_grids_tensor = torch.tensor(np.array(FINAL_X_DL), dtype=torch.float32)\n",
    "    X_phys_arr = np.array(FINAL_X_PHYS)\n",
    "    y_arr = np.array(FINAL_Y)\n",
    "    \n",
    "    print(f\"\\n[PHẦN 2] Training Autoencoder & Ensemble trên {len(y_arr)} mẫu...\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ae_model = Conv1DAutoencoder().to(device)\n",
    "    loader = DataLoader(TensorDataset(X_grids_tensor), batch_size=256, shuffle=True)\n",
    "    optimizer = optim.Adam(ae_model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        for batch in loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = ae_model(inputs); loss = criterion(out, inputs)\n",
    "            loss.backward(); optimizer.step()\n",
    "            \n",
    "    ae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_dl_features = ae_model.get_latent(X_grids_tensor.to(device))\n",
    "\n",
    "    ensemble = HybridEnsemble(scale_pos_weight=(np.sum(y_arr==0)/(np.sum(y_arr==1)+1e-6)))\n",
    "    ensemble.fit(X_phys_arr, X_dl_features, y_arr)\n",
    "\n",
    "    # PHẦN 4: TEST (Dùng Parallel im lặng tương tự)\n",
    "    print(f\"\\n[PHẦN 3] Dự đoán Test (Dùng 100% CPU)...\")\n",
    "    ALL_TEST_DL, ALL_TEST_PHYS, ALL_TEST_IDS = [], [], []\n",
    "    for split_name in split_dirs:\n",
    "        T_LC_PATH = os.path.join(BASE_DIR, split_name, 'test_full_lightcurves.csv')\n",
    "        if not os.path.exists(T_LC_PATH): continue\n",
    "        print(f\"  >> Đang chạy Split: {split_name}...\")\n",
    "        test_lc = test_processor.load_and_process(T_LC_PATH)\n",
    "        test_lc.rename(columns={next(c for c in test_lc.columns if 'mjd' in c.lower() or 'time' in c.lower()): 'Time'}, inplace=True)\n",
    "        \n",
    "        p_inputs = [(oid, test_lc[test_lc['object_id']==oid].copy(), 0, 0) for oid in test_lc['object_id'].unique()]\n",
    "        b_res = Parallel(n_jobs=-1)(delayed(process_single_object)(pi, pd, pt, pa, physics_extractor) for pi, pd, pt, pa in p_inputs)\n",
    "        \n",
    "        for r in b_res:\n",
    "            if r:\n",
    "                ALL_TEST_DL.append(r[0]['dl_feat']); ALL_TEST_PHYS.append(r[0]['phys_feat']); ALL_TEST_IDS.append(r[0]['obj_id'])\n",
    "        del test_lc; gc.collect()\n",
    "\n",
    "    # PHẦN 5: GHI SUBMISSION\n",
    "    X_test_tensor = torch.tensor(np.array(ALL_TEST_DL), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        X_test_latent = ae_model.get_latent(X_test_tensor.to(device))\n",
    "    probs = ensemble.predict_proba(np.array(ALL_TEST_PHYS), X_test_latent)\n",
    "    \n",
    "    sub = pd.DataFrame({'object_id': ALL_TEST_IDS, 'target': probs})\n",
    "    SAMPLE = os.path.join(BASE_DIR, 'sample_submission.csv')\n",
    "    if os.path.exists(SAMPLE):\n",
    "        sub = pd.read_csv(SAMPLE)[['object_id']].merge(sub, on='object_id', how='left').fillna(0)\n",
    "    sub.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "    print(\"✅ HOÀN THÀNH!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
