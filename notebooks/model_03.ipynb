{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588a8a8d",
   "metadata": {},
   "source": [
    "# DATA FOUNDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import gc # ƒê·ªÉ gi·∫£i ph√≥ng RAM\n",
    "\n",
    "BASE_DIR = \"/MALLORN-Astronomical-Classification-Challenge/data/raw\"\n",
    "\n",
    "R_COEFFS = {'u': 4.239, 'g': 3.303, 'r': 2.285, 'i': 1.698, 'z': 1.263, 'y': 1.086}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log(base_dir, mode='train'):\n",
    "    log_path = os.path.join(base_dir, f\"{mode}_log.csv\")\n",
    "    if not os.path.exists(log_path):\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file {mode}_log.csv\")\n",
    "    \n",
    "    df_log=pd.read_csv(log_path)\n",
    "    if mode == 'test':\n",
    "        if 'target' not in df_log.columns:\n",
    "            df_log['target'] = np.nan\n",
    "        if 'Z_err' in df_log.columns:\n",
    "            df_log['Z_err'] = 0\n",
    "    return df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    n_original = len(df)\n",
    "\n",
    "    # 1. X·ª© l√Ω NaN\n",
    "    # N·∫øu Flux ho·∫∑c Flux_err b·ªã NaN -> X√≥a d√≤ng v√¨ d·ªØ li·ªáu v√¥ nghƒ©a\n",
    "    df = df.dropna(subset=['Flux', 'Flux_err', 'Flux_corrected', 'Flux_err_corrected'])\n",
    "\n",
    "    # N·∫øu EBV b·ªã NaN, ƒëi·ªÅn b·∫±ng 0 (coi nh∆∞ kh√¥ng c√≥ b·ª•i) ƒë·ªÉ tr√°nh l·ªói t√≠nh to√°n\n",
    "    if 'EBV' in df.columns:\n",
    "        df['EBV'] = df['EBV'].fillna(0)\n",
    "\n",
    "    # 2. X·ª≠ l√Ω Flux √¢m\n",
    "    # Kh√¥ng x√≥a FLux √¢m nh∆∞ng ki·ªÉm k√™ n√≥\n",
    "    n_negative = (df['Flux_corrected'] < 0).sum()\n",
    "\n",
    "    # 3. S·∫Øp x·∫øp th·ªùi gian\n",
    "    df = df.sort_values(by=['object_id', 'Time (MJD)'], ascending=[True, True])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    n_dropped = n_original - len(df)\n",
    "    if n_dropped > 0:\n",
    "        print(f\"   ƒê√£ lo·∫°i b·ªè {n_dropped} d√≤ng ch·ª©a NaN.\")\n",
    "    if n_negative > 0:\n",
    "        print(f\"   L∆∞u √Ω: C√≥ {n_negative} ƒëi·ªÉm ƒëo c√≥ Flux √Çm (v·∫´n gi·ªØ l·∫°i).\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61457bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_split(split_name, df_log, base_dir, mode='train'):\n",
    "    print(f\"\\n ƒêang x·ª≠ l√Ω: {split_name}...\")\n",
    "    \n",
    "    lc_path = os.path.join(base_dir, split_name, f\"{mode}_full_lightcurves.csv\")\n",
    "    \n",
    "    df_meta_split = df_log[df_log['split'] == split_name].copy()\n",
    "    \n",
    "    df_lc = pd.read_csv(lc_path)\n",
    "\n",
    "    df_lc_merged = df_lc.merge(\n",
    "        df_meta_split[['object_id', 'EBV', 'target']],\n",
    "        on='object_id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    df_lc_merged['R_factor'] = df_lc_merged['Filter'].map(R_COEFFS)\n",
    "    correction = 10 ** (0.4 * df_lc_merged['R_factor'] * df_lc_merged['EBV'])\n",
    "    df_lc_merged['Flux_corrected'] = df_lc_merged['Flux'] * correction\n",
    "    df_lc_merged['Flux_err_corrected'] = df_lc_merged['Flux_err'] * correction\n",
    "\n",
    "    df_lc_clean = clean_data(df_lc_merged)\n",
    "    del df_lc, df_lc_merged\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"   -> Ho√†n t·∫•t {split_name}\")\n",
    "    return df_lc_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b7994",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b6ee5",
   "metadata": {},
   "source": [
    "STATISTICAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistical_features(df):\n",
    "    print(\"ƒêang t·∫°o: Statistical & Percentiles Features....\")\n",
    "\n",
    "    # 1. ƒê·ªãnh nghƒ©a c√°c h√†m th·ªëng k√™\n",
    "    aggregations = {\n",
    "        'Flux_corrected': [\n",
    "            'mean', 'std', 'max', 'min',\n",
    "            ('q05', lambda x: x.quantile(0.05)), # Ph√¢n v·ªã 5% (thay cho Min ƒë·ªÉ tr√°nh nhi·ªÖu)\n",
    "            ('q25', lambda x: x.quantile(0.25)),\n",
    "            ('q75', lambda x: x.quantile(0.75)),\n",
    "            ('q95', lambda x: x.quantile(0.95)), # Ph√¢n v·ªã 95% (thay cho Max ƒë·ªÉ tr√°nh nhi·ªÖu)\n",
    "            'skew', # ƒê·ªô l·ªách ph√¢n ph·ªëi (quan tr·ªçng cho TDE)\n",
    "            'count' # S·ªë l∆∞·ª£ng ƒëi·ªÉm ƒëo\n",
    "        ],\n",
    "        'Flux': ['max'], # Gi·ªØ l·∫°i max flux g·ªëc ƒë·ªÉ tham chi·∫øu\n",
    "        'Flux_err': ['mean'] # Sai s·ªë trung b√¨nh (ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ƒëo)\n",
    "    }\n",
    "\n",
    "    # 2. Groupby v√† Aggregation (Th·ª±c hi·ªán song song cho t·∫•t c·∫£ object)\n",
    "    # Bi·∫øn ƒë·ªïi b·∫£ng Long-fomr th√†nh b·∫£ng th·ªëng k√™ s∆° b·ªô\n",
    "    df_agg = df.groupby(['object_id', 'Filter']).agg(aggregations)\n",
    "\n",
    "    # 3. L√†m ph·∫£ng MultiIndex Columns\n",
    "    df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n",
    "\n",
    "    # 4. Unstack (Xoay tr·ª•c Filter th√†nh c·ªôt)\n",
    "    df_features = df_agg.unstack('Filter')\n",
    "    df_features.columns = [f\"{filter_name}_{feature}\" for feature, filter_name in df_features.columns]\n",
    "\n",
    "    # 5. T√≠nh c√°c ƒë·∫∑c tr∆∞ng \"Global\" (T·ªïng h·ª£p tr√™n m·ªçi band)\n",
    "    global_agg = df.groupby('object_id')['Flux_corrected'].agg([\n",
    "        ('global_max', 'max'),\n",
    "        ('global_mean', 'mean'),\n",
    "        ('global_std', 'std')\n",
    "    ])\n",
    "\n",
    "    # Gh√©p global v√†o b·∫£ng features\n",
    "    df_features = df_features.join(global_agg)\n",
    "\n",
    "    # 6. X·ª≠ l√Ω NaN sinh ra do Unstack\n",
    "    # (V√≠ d·ª•: Object A kh√¥ng c√≥ d·ªØ li·ªáu filter 'u', s·∫Ω sinh ra NaN t·∫°i c√°c c·ªôt u_...)\n",
    "    # Chi·∫øn thu·∫≠t: \n",
    "    # - V·ªõi Mean, Max, Min, Quantile: Fill = 0 (coi nh∆∞ t·ªëi ƒëen)\n",
    "    # - V·ªõi Count: Fill = 0\n",
    "    df_features = df_features.fillna(0)\n",
    "\n",
    "    print(f\"‚úÖ Ho√†n th√†nh Nh√≥m 1! K√≠ch th∆∞·ªõc: {df_features.shape}\")\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fd3c1",
   "metadata": {},
   "source": [
    "MORPHOLOGY FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_morphology_features(df_clean, df_features_g1):\n",
    "    print(\"   ƒêang t·∫°o: Morphology Features\")\n",
    "\n",
    "    # index c·ªßa d√≤ng c√≥ Flux l·ªõn nh·∫•t cho m·ªói object/filter\n",
    "    idx_max = df_clean.groupby(['object_id', 'Filter'])['Flux_corrected'].idxmax()\n",
    "\n",
    "    # index c·ªßa d√≤ng ƒë·∫ßu ti√™n v√† d√≤ng cu·ªëi c√πng (theo th·ªùi gian)\n",
    "    idx_first = df_clean.groupby(['object_id', 'Filter'])['Time (MJD)'].idxmin()\n",
    "    idx_last = df_clean.groupby(['object_id', 'Filter'])['Time (MJD)'].idxmax()\n",
    "\n",
    "     # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·∫°i c√°c ƒëi·ªÉm m·ªëc\n",
    "    df_peaks = df_clean.loc[idx_max][['object_id', 'Filter', 'Time (MJD)', 'Flux_corrected']].set_index(['object_id', 'Filter'])\n",
    "    df_starts = df_clean.loc[idx_first][['object_id', 'Filter', 'Time (MJD)', 'Flux_corrected']].set_index(['object_id', 'Filter'])\n",
    "    df_ends = df_clean.loc[idx_last][['object_id', 'Filter', 'Time (MJD)', 'Flux_corrected']].set_index(['object_id', 'Filter'])\n",
    "\n",
    "    df_peaks.columns = ['Time_peak', 'Flux_peak']\n",
    "    df_starts.columns = ['Time_start', 'Flux_start']\n",
    "    df_ends.columns = ['Time_end', 'Flux_end']\n",
    "\n",
    "    # Gh√©p l·∫°i th√†nh m·ªôt b·∫£ng cho morphology\n",
    "    df_morp = pd.concat([df_peaks, df_starts, df_ends], axis=1)\n",
    "\n",
    "    # T√≠nh Time, Slope\n",
    "    df_morp['Rise_time'] = df_morp['Time_peak'] - df_morp['Time_start']\n",
    "    df_morp['Fall_time'] = df_morp['Time_end'] - df_morp['Time_peak']\n",
    "    df_morp['Rise_slope'] = (df_morp['Flux_peak'] - df_morp['Flux_start']) / (df_morp['Rise_time'] + 0.1)\n",
    "    df_morp['Fall_slope'] = (df_morp['Flux_end'] - df_morp['Flux_peak']) / (df_morp['Fall_time'] + 0.1)\n",
    "    \n",
    "    cols_time_slope = ['Rise_time', 'Fall_time', 'Rise_slope', 'Fall_slope']\n",
    "    df_morp_ts = df_morp[cols_time_slope].unstack('Filter')\n",
    "    df_morp_ts.columns = [f\"{filter_name}_{feat}\" for feat, filter_name in df_morp_ts.columns]\n",
    "\n",
    "    # T√≠nh Percent Amplitude\n",
    "    amp_features = []\n",
    "    filters = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "    \n",
    "    for f in filters:\n",
    "        col_max = f\"{f}_Flux_corrected_max\"\n",
    "        col_min = f\"{f}_Flux_corrected_min\"\n",
    "        col_mean = f\"{f}_Flux_corrected_mean\"\n",
    "\n",
    "        amp_series = (df_features_g1[col_max] - df_features_g1[col_min]) / (df_features_g1[col_mean] + 1e-6)\n",
    "        amp_df = pd.DataFrame({f\"{f}_percent_amplitude\": amp_series})\n",
    "        amp_features.append(amp_df)\n",
    "    df_morp_amp = pd.concat(amp_features, axis=1)\n",
    "\n",
    "    # T√≠nh Kurtosis\n",
    "    df_morp_kurt = df_clean.groupby(['object_id', 'Filter'])['Flux_corrected'].apply(lambda x: x.kurt()).unstack('Filter')\n",
    "    df_morp_kurt.columns = [f\"{f}_kurtosis\" for f in df_morp_kurt.columns]\n",
    "\n",
    "    # T√≠nh Stetson K\n",
    "    mean_cols = [c for c in df_features_g1.columns if 'mean' in c and 'Flux_corrected' in c and 'global' not in c]\n",
    "    df_means = df_features_g1[mean_cols].copy()\n",
    "    df_means.columns = [c.split('_')[0] for c in df_means.columns]\n",
    "\n",
    "    df_means_flat = df_means.stack().reset_index()\n",
    "    df_means_flat.columns = ['object_id', 'Filter', 'mean_flux']\n",
    "    df_stetson = pd.merge(df_clean, df_means_flat, on=['object_id', 'Filter'], how='left')\n",
    "\n",
    "    df_stetson['delta'] = (df_stetson['Flux_corrected'] - df_stetson['mean_flux']) / (df_stetson['Flux_err_corrected'] + 1e-6)\n",
    "    \n",
    "    df_stetson['abs_delta'] = df_stetson['delta'].abs()\n",
    "    df_stetson['delta_sq'] = df_stetson['delta'] ** 2\n",
    "\n",
    "    g = df_stetson.groupby(['object_id', 'Filter'])\n",
    "    mean_abs_delta = g['abs_delta'].mean()\n",
    "    mean_delta_sq = g['delta_sq'].mean()\n",
    "    stetson_k = mean_abs_delta / (np.sqrt(mean_delta_sq) + 1e-6)\n",
    "\n",
    "    df_morp_stetson = stetson_k.unstack('Filter')\n",
    "    df_morp_stetson.columns = [f\"{f}_stetson_k\" for f in df_morp_stetson.columns]\n",
    "\n",
    "\n",
    "    # T·∫°o b·∫£ng morphology features\n",
    "    df_morp_final = df_morp_ts.join([df_morp_amp, df_morp_kurt, df_morp_stetson])\n",
    "\n",
    "    # X·ª≠ l√Ω NaN n·∫øu c√≥\n",
    "    df_morp_final = df_morp_final.fillna(0)\n",
    "\n",
    "    print(f\"Ho√†n th√†nh Nh√≥m 2! K√≠ch th∆∞·ªõc: {df_morp_final.shape}\")\n",
    "    return df_morp_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00112a62",
   "metadata": {},
   "source": [
    "COLOR FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5395c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color_features(df_features_g1):\n",
    "    print(\"   ƒêang t·∫°o: Color Features\")\n",
    "\n",
    "    filters = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "    new_features = []\n",
    "\n",
    "    for i in range(len(filters) - 1):\n",
    "        f1 = filters[i]\n",
    "        f2 = filters[i+1]\n",
    "\n",
    "        col_f1_mean = f\"{f1}_Flux_corrected_mean\"\n",
    "        col_f2_mean = f\"{f2}_Flux_corrected_mean\"\n",
    "\n",
    "        ratio_mean = df_features_g1[col_f1_mean] / (df_features_g1[col_f2_mean] + 1e-6)\n",
    "        diff_mean = df_features_g1[col_f1_mean] - df_features_g1[col_f2_mean]\n",
    "\n",
    "        col_f1_max = f\"{f1}_Flux_corrected_max\"\n",
    "        col_f2_max = f\"{f2}_Flux_corrected_max\"\n",
    "\n",
    "        ratio_max = df_features_g1[col_f1_max] / (df_features_g1[col_f2_max] + 1e-6)\n",
    "        diff_max = df_features_g1[col_f1_max] - df_features_g1[col_f2_max]\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            f'{f1}_{f2}_mean_ratio': ratio_mean,\n",
    "            f'{f1}_{f2}_mean_diff': diff_mean,\n",
    "            f'{f1}_{f2}_max_ratio': ratio_max,\n",
    "            f'{f1}_{f2}_max_diff': diff_max\n",
    "        }, index=df_features_g1.index)\n",
    "\n",
    "        new_features.append(temp_df)\n",
    "\n",
    "    f_start, f_end = 'u', 'y'\n",
    "    col_start = f\"{f_start}_Flux_corrected_mean\"\n",
    "    col_end = f\"{f_end}_Flux_corrected_mean\"\n",
    "\n",
    "    blue_red_ratio = df_features_g1[col_start] / (df_features_g1[col_end] + 1e-6)\n",
    "    temp_extreme = pd.DataFrame({f'{f_start}_{f_end}_mean_ratio': blue_red_ratio}, index=df_features_g1.index)\n",
    "    new_features.append(temp_extreme)\n",
    "\n",
    "    df_colors = pd.concat(new_features, axis=1)\n",
    "    df_colors = df_colors.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    print(f\"Ho√†n th√†nh Nh√≥m 3! K√≠ch th∆∞·ªõc: {df_colors.shape}\")\n",
    "    return df_colors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c6b8a",
   "metadata": {},
   "source": [
    "BAZIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "import warnings\n",
    "\n",
    "# T·∫Øt c·∫£nh b√°o t√≠nh to√°n (overflow) ƒë·ªÉ log ƒë·ª° r√°c\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def bazin_func(params, t):\n",
    "    \"\"\"\n",
    "    params: [A, B, t0, tau_rise, tau_fall]\n",
    "    t: m·∫£ng th·ªùi gian\n",
    "    \"\"\"\n",
    "    A, B, t0, tau_rise, tau_fall = params\n",
    "\n",
    "    arg_fall = -(t - t0) / (tau_fall + 1e-5)\n",
    "    arg_rise = -(t - t0) / (tau_rise + 1e-5)\n",
    "\n",
    "    return A * (np.exp(arg_fall) / (1 + np.exp(arg_rise))) + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(params, t, flux, flux_err):\n",
    "    \"\"\"\n",
    "    H√†m t√≠nh sai s·ªë ƒë·ªÉ t·ªëi ∆∞u h√≥a.\n",
    "    M·ª•c ti√™u: Minimizing (D·ªØ li·ªáu th·ª±c - D·ªØ li·ªáu bazin) / Sai s·ªë ƒëo\n",
    "    \"\"\"\n",
    "    model = bazin_func(params, t)\n",
    "    weights = 1.0 / (flux_err + 1e-5)\n",
    "    return (flux - model) * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bazin_one_series(time, flux, flux_err):\n",
    "    \"\"\"\n",
    "    H√†m th·ª±c hi·ªán fit cho 1 chu·ªói th·ªùi gian (1 filter c·ªßa 1 object)\n",
    "    Tr·∫£ v·ªÅ: [A, B, t0, tau_rise, tau_fall] v√† fit_error\n",
    "    \"\"\"\n",
    "\n",
    "    # Chuy·ªÉn v·ªÅ numpy array v√† sort theo th·ªùi gian\n",
    "    idx = np.argsort(time)\n",
    "    t = time[idx]\n",
    "    f = flux[idx]\n",
    "    e = flux_err[idx]\n",
    "\n",
    "    # Chu·∫©n h√≥a th·ªùi gian ƒë·ªÉ t√≠nh to√°n ·ªïn ƒë·ªãnh h∆°n (t b·∫Øt ƒë·∫ßu t·ª´ 0)\n",
    "    t_min = t.min()\n",
    "    t_norm = t - t_min\n",
    "\n",
    "    # D·ª± ƒëo√°n tham s·ªë kh·ªüi t·∫°o (N·∫øu ƒëo√°n sai, least_squares s·∫Ω kh√¥ng h·ªôi t·ª•)\n",
    "    max_flux_idx = np.argmax(f)\n",
    "    max_flux = f[max_flux_idx]\n",
    "\n",
    "    guess_A = max_flux # Bi√™n ƒë·ªô ~ Flux cao nh·∫•t\n",
    "    guess_B = np.median(f[:3]) if len(f) > 3 else 0 # N·ªÅn ~ gi√° tr·ªã ƒëo·∫°n ƒë·∫ßu\n",
    "    guess_t0 = t_norm[max_flux_idx] # ƒê·ªânh ~ th·ªùi ƒëi·ªÉm flux cao nh·∫•t\n",
    "    guess_rise = 10.0 # Gi·∫£ ƒë·ªãnh ban ƒë·∫ßu: tƒÉng trong 10 gi√¢y\n",
    "    guess_fall = 30.0 # Gi·∫£ ƒë·ªãnh band d·∫ßu: gi·∫£m trong 30 gi√¢y\n",
    "\n",
    "    x0 = [guess_A, guess_B, guess_t0, guess_rise, guess_fall]\n",
    "\n",
    "    # Gi·ªõi h·∫°n tham s·ªë\n",
    "    # A > 0, Tau > 0. t0 n·∫±m trong kho·∫£ng th·ªùi gian quan s√°t\n",
    "    lower_bounds = [0, -np.inf, t_norm.min()-50, 0.1, 0.1]\n",
    "    upper_bounds = [np.inf, np.inf, t_norm.max()+50, 200, 500]\n",
    "\n",
    "    try:\n",
    "        # Ch·∫°y t·ªëi ∆∞u h√≥a\n",
    "        res = least_squares(\n",
    "            residuals,\n",
    "            x0,\n",
    "            args=(t_norm, f, e),\n",
    "            bounds=(lower_bounds, upper_bounds),\n",
    "            method='trf', # Trust Region Reflective\n",
    "            loss='soft_l1' # Gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa Outliers\n",
    "        )\n",
    "\n",
    "        final_params = res.x\n",
    "        final_params[2] += t_min\n",
    "\n",
    "        # T√≠nh chi square (ƒë·ªô t·ªët c·ªßa fit)\n",
    "        chisq = np.sum(res.fun**2) / (len(f)-5)\n",
    "\n",
    "        return list(final_params) + [chisq]\n",
    "    except Exception:\n",
    "        # N·∫øu l·ªói, tr·∫£ v·ªÅ NaN\n",
    "        return [np.nan] * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bazin_features(df_clean):\n",
    "    print(\"ƒêang ch·∫°y Bazin Parametric Fitting....\")\n",
    "\n",
    "    features = []\n",
    "    object_ids = df_clean['object_id'].unique()\n",
    "    filters = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "\n",
    "    # L·∫∑p qua t·ª´ng object. ƒê·ªÉ t·ªëi ∆∞u, sau n√†y c√≥ th·ªÉ d√πng jobLib Parallel\n",
    "    cnt = 0\n",
    "    total = len(object_ids)\n",
    "\n",
    "    for obj_id in object_ids:\n",
    "        df_obj = df_clean[df_clean['object_id'] == obj_id]\n",
    "\n",
    "        obj_feats = {'object_id' : obj_id}\n",
    "\n",
    "        for flt in filters:\n",
    "            df_flt = df_obj[df_obj['Filter'] == flt]\n",
    "\n",
    "            if len(df_flt) < 5:\n",
    "                # Kh√¥ng ƒë·ªß ƒëi·ªÉm ƒë·ªÉ fit (c·∫ßn t·ªëi thi·ªÉu 5 ƒëi·ªÉm cho 5 tham s·ªë)\n",
    "                params = [np.nan] * 6\n",
    "            else:\n",
    "                params = fit_bazin_one_series(\n",
    "                    df_flt['Time (MJD)'].values,\n",
    "                    df_flt['Flux_corrected'].values,\n",
    "                    df_flt['Flux_err_corrected'].values\n",
    "                )\n",
    "            \n",
    "            # L∆∞u k√©t qu·∫£: u_banzin_A, u_bazin_tau_rise, ...\n",
    "            prefix = f\"{flt}_bazin\"\n",
    "            obj_feats[f\"{prefix}_A\"] = params[0]\n",
    "            obj_feats[f\"{prefix}_B\"] = params[1]\n",
    "            obj_feats[f\"{prefix}_t0\"] = params[2]\n",
    "            obj_feats[f\"{prefix}_tau_rise\"] = params[3]\n",
    "            obj_feats[f\"{prefix}_tau_fall\"] = params[4]\n",
    "            obj_feats[f\"{prefix}_chisq\"] = params[5]\n",
    "        \n",
    "        features.append(obj_feats)\n",
    "\n",
    "        cnt += 1\n",
    "        if cnt % 50 == 0:\n",
    "            print(f\"   -> ƒê√£ fit xong {cnt}/{total} v·∫≠t th·ªÉ.\")\n",
    "        \n",
    "    df_bazin = pd.DataFrame(features)\n",
    "    df_bazin.set_index('object_id', inplace=True)\n",
    "\n",
    "    df_bazin = df_bazin.fillna(-1)\n",
    "\n",
    "    print(f\"Ho√†n t·∫•t Bazin Fitting! K√≠ch th∆∞·ªõc: {df_bazin.shape}\")\n",
    "\n",
    "    return df_bazin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15761faf",
   "metadata": {},
   "source": [
    "# PROCESS TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e034df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "PROCESSED_DIR = \"/MALLORN-Astronomical-Classification-Challenge/data/processed\"\n",
    "PROCESSED_TRAIN_FILE = \"processed_train_full_lc.csv\"\n",
    "\n",
    "df_train_log = load_log(BASE_DIR, mode='train')\n",
    "processed_chunks = []\n",
    "\n",
    "print(\"--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 20 splits---\")\n",
    "\n",
    "for i in range(1, 21):\n",
    "    split_name = f\"split_{i:02d}\"\n",
    "    print(f\"ƒêang x·ª≠ l√Ω: {split_name} ({i}/20)...\")\n",
    "\n",
    "    df_clean = process_one_split(split_name, df_train_log, BASE_DIR, mode='train')\n",
    "    if df_clean is None:\n",
    "        continue\n",
    "\n",
    "    df_g1 = generate_statistical_features(df_clean)\n",
    "    df_g2 = generate_morphology_features(df_clean, df_g1)\n",
    "    df_g3 = generate_color_features(df_g1)\n",
    "    df_g4 = generate_bazin_features(df_clean)\n",
    "\n",
    "    current_ids = df_g1.index\n",
    "    df_train_log_tmp = df_train_log[df_train_log['object_id'].isin(current_ids)].copy()\n",
    "\n",
    "    df_temp = pd.concat([df_g1, df_g2, df_g3], axis=1)\n",
    "    df_temp = df_temp.join(df_g4, how='left')\n",
    "    df_chunk_final = df_train_log_tmp.set_index('object_id').join(df_temp, how='inner')\n",
    "    \n",
    "    processed_chunks.append(df_chunk_final)\n",
    "\n",
    "    print(f\"   -> X·ª≠ l√Ω xong {split_name}. K√≠ch th∆∞·ªõc chunk: {df_chunk_final.shape}\")\n",
    "\n",
    "    # D·ªçn d·∫πp RAM\n",
    "    del df_clean, df_g1, df_g2, df_g3, df_g4, df_chunk_final, df_train_log_tmp\n",
    "    gc.collect()\n",
    "\n",
    "print(\"ƒêang gh√©p n·ªëi t·∫•t c·∫£ c√°c splits...\")\n",
    "if (len(processed_chunks) > 0):\n",
    "    df_final_train_lc = pd.concat(processed_chunks, axis=0)\n",
    "    print(f\"T·ªïng k√≠ch th∆∞·ªõc: {df_final_train_lc.shape}\")\n",
    "    print(f\"T·ªïng s·ªë TDE t√¨m th·∫•y: {df_final_train_lc['target'].sum()}\")\n",
    "\n",
    "    # L∆∞u l·∫°i file trong processed\n",
    "    save_path = os.path.join(PROCESSED_DIR, PROCESSED_TRAIN_FILE)\n",
    "    df_final_train_lc.to_csv(save_path, index=True)\n",
    "    print(f\"ƒê√£ l∆∞u file t·∫°i: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f1f09",
   "metadata": {},
   "source": [
    "# PROCESS TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_TEST_FILE = \"processed_test_full_lc.csv\"\n",
    "\n",
    "df_test_log = load_log(BASE_DIR, mode='test')\n",
    "processed_chunks = []\n",
    "\n",
    "print(\"--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 20 splits---\")\n",
    "\n",
    "for i in range(1, 21):\n",
    "    split_name = f\"split_{i:02d}\"\n",
    "    print(f\"ƒêang x·ª≠ l√Ω: {split_name} ({i}/20)...\")\n",
    "\n",
    "    df_clean = process_one_split(split_name, df_test_log, BASE_DIR, mode='test')\n",
    "    if df_clean is None:\n",
    "        continue\n",
    "\n",
    "    df_g1 = generate_statistical_features(df_clean)\n",
    "    df_g2 = generate_morphology_features(df_clean, df_g1)\n",
    "    df_g3 = generate_color_features(df_g1)\n",
    "    df_g4 = generate_bazin_features(df_clean)\n",
    "\n",
    "    current_ids = df_g1.index\n",
    "    df_test_log_tmp = df_test_log[df_test_log['object_id'].isin(current_ids)].copy()\n",
    "\n",
    "    df_temp = pd.concat([df_g1, df_g2, df_g3], axis=1)\n",
    "    df_temp = df_temp.join(df_g4, how='left')\n",
    "    df_chunk_final = df_test_log_tmp.set_index('object_id').join(df_temp, how='inner')\n",
    "    \n",
    "    processed_chunks.append(df_chunk_final)\n",
    "\n",
    "    print(f\"   -> X·ª≠ l√Ω xong {split_name}. K√≠ch th∆∞·ªõc chunk: {df_chunk_final.shape}\")\n",
    "\n",
    "    # D·ªçn d·∫πp RAM\n",
    "    del df_clean, df_g1, df_g2, df_g3, df_g4, df_chunk_final, df_test_log_tmp\n",
    "    gc.collect()\n",
    "\n",
    "print(\"ƒêang gh√©p n·ªëi t·∫•t c·∫£ c√°c splits...\")\n",
    "if (len(processed_chunks) > 0):\n",
    "    df_final_test_lc = pd.concat(processed_chunks, axis=0)\n",
    "\n",
    "    # L∆∞u l·∫°i file trong processed\n",
    "    save_path = os.path.join(PROCESSED_DIR, PROCESSED_TEST_FILE)\n",
    "    df_final_test_lc.to_csv(save_path, index=True)\n",
    "    print(f\"ƒê√£ l∆∞u file t·∫°i: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77c57a",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "152c68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "class AdvancedFeatureSelector:\n",
    "    def __init__(self, variance_thresh=0.0, correlation_thresh=0.98, max_features=140):\n",
    "        self.var_thresh = variance_thresh\n",
    "        self.corr_thresh = correlation_thresh\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        # L∆∞u danh s√°ch c√°c c·ªôt s·∫Ω b·ªè\n",
    "        self.drop_cols_var = []\n",
    "        self.drop_cols_corr = []\n",
    "        self.drop_cols_model = []\n",
    "        self.final_cols = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print(\"üõ°Ô∏è [Smart Selector] B·∫Øt ƒë·∫ßu quy tr√¨nh l·ªçc features...\")\n",
    "        \n",
    "        # --- B∆Ø·ªöC 1: Variance Threshold ---\n",
    "        # T√¨m c√°c c·ªôt c√≥ ph∆∞∆°ng sai th·∫•p (h·∫±ng s·ªë)\n",
    "        selector_var = VarianceThreshold(threshold=self.var_thresh)\n",
    "        selector_var.fit(X)\n",
    "        # L·∫•y mask (True l√† gi·ªØ, False l√† b·ªè)\n",
    "        mask_var = selector_var.get_support()\n",
    "        self.drop_cols_var = X.columns[~mask_var].tolist()\n",
    "        \n",
    "        X_v = X.loc[:, mask_var]\n",
    "        print(f\"   1. Variance: Lo·∫°i {len(self.drop_cols_var)} c·ªôt h·∫±ng s·ªë. C√≤n l·∫°i: {X_v.shape[1]}\")\n",
    "        \n",
    "        # --- B∆Ø·ªöC 2: Correlation Filter ---\n",
    "        # T√≠nh ma tr·∫≠n t∆∞∆°ng quan\n",
    "        print(f\"   2. Correlation: ƒêang t√≠nh to√°n ma tr·∫≠n t∆∞∆°ng quan (> {self.corr_thresh})...\")\n",
    "        corr_matrix = X_v.corr().abs()\n",
    "        \n",
    "        # Ch·ªçn tam gi√°c tr√™n c·ªßa ma tr·∫≠n\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # T√¨m c√°c c·ªôt c√≥ correlation > threshold\n",
    "        self.drop_cols_corr = [column for column in upper.columns if any(upper[column] > self.corr_thresh)]\n",
    "        \n",
    "        X_c = X_v.drop(columns=self.drop_cols_corr)\n",
    "        print(f\"      -> Lo·∫°i b·ªè {len(self.drop_cols_corr)} c·ªôt tr√πng l·∫∑p th√¥ng tin. C√≤n l·∫°i: {X_c.shape[1]}\")\n",
    "        \n",
    "        # --- B∆Ø·ªöC 3: Model-based Selection ---\n",
    "        print(f\"   3. Importance: D√πng LightGBM ƒë·ªÉ ch·ªçn Top {self.max_features} features...\")\n",
    "        \n",
    "        # Ch·ªâ ch·∫°y b∆∞·ªõc n√†y n·∫øu s·ªë feature hi·ªán t·∫°i > max_features\n",
    "        if X_c.shape[1] > self.max_features:\n",
    "            lgb = LGBMClassifier(n_estimators=200, learning_rate=0.05, random_state=42, n_jobs=-1, verbose=-1)\n",
    "            lgb.fit(X_c, y)\n",
    "            \n",
    "            selector_model = SelectFromModel(lgb, max_features=self.max_features, prefit=True)\n",
    "            mask_model = selector_model.get_support()\n",
    "            \n",
    "            self.final_cols = X_c.columns[mask_model].tolist()\n",
    "            self.drop_cols_model = [c for c in X_c.columns if c not in self.final_cols]\n",
    "            \n",
    "            print(f\"      -> Lo·∫°i b·ªè {len(self.drop_cols_model)} c·ªôt y·∫øu. Gi·ªØ l·∫°i {len(self.final_cols)} features tinh t√∫y.\")\n",
    "        else:\n",
    "            self.final_cols = X_c.columns.tolist()\n",
    "            print(f\"      -> S·ªë features hi·ªán t·∫°i ({X_c.shape[1]}) <= max_features. Gi·ªØ nguy√™n.\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ch·ªâ vi·ªác l·∫•y ƒë√∫ng danh s√°ch c·ªôt cu·ªëi c√πng\n",
    "        return X[self.final_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783afc7",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc383208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3413d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/MALLORN-Astronomical-Classification-Challenge/data/processed/processed_train_full_lc.csv\")\n",
    "df_test = pd.read_csv(\"/MALLORN-Astronomical-Classification-Challenge/data/processed/processed_test_full_lc.csv\")\n",
    "\n",
    "df_train = df_train.replace([np.inf, -np.inf], np.nan).fillna(-999)\n",
    "df_test = df_test.replace([np.inf, -np.inf], np.nan).fillna(-999)\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': 0.051518799097782834,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.9106527930168713,\n",
    "    'colsample_bytree': 0.7028192063273316,\n",
    "    'gamma': 0.6560553174292321,\n",
    "    'reg_alpha': 8.328749198565307,\n",
    "    'reg_lambda': 1.2457482664378638,\n",
    "    'scale_pos_weight': 10.692975531517197,\n",
    "    'n_estimators': 1500,\n",
    "    'eval_metric': 'logloss',\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'learning_rate': 0.09910411868030085,\n",
    "    'num_leaves': 51,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.8473086396918044,\n",
    "    'colsample_bytree': 0.6978315039075688,\n",
    "    'reg_alpha': 7.089502551216929,\n",
    "    'reg_lambda': 3.3773533645913636,\n",
    "    'class_weight': 'balanced',\n",
    "    'n_estimators': 1500,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    'learning_rate': 0.01844791176169218,\n",
    "    'depth': 4,\n",
    "    'l2_leaf_reg': 2.082185084558599,\n",
    "    'random_strength': 5.209528399826417,\n",
    "    'bagging_temperature': 0.5151147661811961,\n",
    "    'scale_pos_weight': 7.369664685132736,\n",
    "    'iterations': 1500,\n",
    "    'loss_function': 'Logloss',\n",
    "    'verbose': False,\n",
    "    'random_seed': 42,\n",
    "    'allow_writing_files': False,\n",
    "    'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "weakness_features = ['global_max', 'i_kurtosis', 'z_kurtosis', \n",
    "                     'y_Flux_corrected_min', 'r_Flux_corrected_mean', \n",
    "                     'z_y_mean_ratio', 'z_Flux_corrected_q75', 'y_bazin_A', \n",
    "                     'z_stetson_k', 'y_Flux_corrected_q75', 'g_Rise_slope', \n",
    "                     'y_Rise_slope', 'u_Rise_slope', 'i_Flux_corrected_mean', \n",
    "                     'i_Flux_corrected_std', 'g_Flux_corrected_count', \n",
    "                     'i_Flux_corrected_count', 'y_Rise_time', \n",
    "                     'g_Flux_corrected_mean', 'Z_err']\n",
    "\n",
    "golden_features = [\n",
    "    'Z', 'u_Flux_corrected_mean','g_Flux_corrected_std','r_Flux_corrected_max',\n",
    "    'g_Flux_corrected_min','u_Flux_corrected_min','g_Flux_corrected_q05',\n",
    "    'r_Flux_corrected_q25','u_Flux_corrected_q25','z_Flux_corrected_q25',\n",
    "    'u_Flux_corrected_q75','z_Flux_corrected_q95','g_Flux_corrected_skew',\n",
    "    'i_Flux_corrected_skew','r_Flux_corrected_skew','y_Flux_corrected_skew',\n",
    "    'u_Flux_err_mean','i_Fall_time','r_Fall_time',\n",
    "    'z_Fall_time','u_Fall_slope','z_Fall_slope',\n",
    "    'u_percent_amplitude','r_percent_amplitude','z_percent_amplitude',\n",
    "    'y_kurtosis','u_stetson_k','y_stetson_k','u_g_max_diff',\n",
    "    'g_r_max_ratio','g_r_max_diff','r_i_mean_diff',\n",
    "    'r_i_max_ratio','r_i_max_diff','i_z_max_diff',\n",
    "    'z_y_mean_diff','u_bazin_B','u_bazin_tau_fall','u_bazin_chisq',\n",
    "    'g_bazin_B','g_bazin_tau_fall','r_bazin_A','r_bazin_B','r_bazin_tau_rise',\n",
    "    'r_bazin_tau_fall','r_bazin_chisq','i_bazin_tau_rise','i_bazin_tau_fall',\n",
    "    'i_bazin_chisq','z_bazin_A','z_bazin_tau_rise','z_bazin_tau_fall',\n",
    "    'z_bazin_chisq','y_bazin_chisq']\n",
    "\n",
    "SEED = 42\n",
    "SEEDS = [42, 2024, 123]\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d21a6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è [Smart Selector] B·∫Øt ƒë·∫ßu quy tr√¨nh l·ªçc features...\n",
      "   1. Variance: Lo·∫°i 1 c·ªôt h·∫±ng s·ªë. C√≤n l·∫°i: 176\n",
      "   2. Correlation: ƒêang t√≠nh to√°n ma tr·∫≠n t∆∞∆°ng quan (> 0.98)...\n",
      "      -> Lo·∫°i b·ªè 7 c·ªôt tr√πng l·∫∑p th√¥ng tin. C√≤n l·∫°i: 169\n",
      "   3. Importance: D√πng LightGBM ƒë·ªÉ ch·ªçn Top 120 features...\n",
      "      -> Lo·∫°i b·ªè 115 c·ªôt y·∫øu. Gi·ªØ l·∫°i 54 features tinh t√∫y.\n",
      " D·ªÆ LI·ªÜU ƒê√É S·∫¥N S√ÄNG: X: (3043, 54), y:148/3043, X_test(7135, 54)\n"
     ]
    }
   ],
   "source": [
    "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'prediction']\n",
    "existing_drop = [c for c in drop_cols if c in df_train.columns]\n",
    "\n",
    "X = df_train.drop(columns=existing_drop)\n",
    "selector = AdvancedFeatureSelector(variance_thresh=0.0, correlation_thresh=0.98, max_features=120)\n",
    "selector.fit(X, y)\n",
    "X = selector.transform(X)\n",
    "X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "y = df_train['target']\n",
    "\n",
    "train_cols = X.columns.tolist()\n",
    "X_test = df_test[train_cols]\n",
    "\n",
    "print(f\" D·ªÆ LI·ªÜU ƒê√É S·∫¥N S√ÄNG: X: {X.shape}, y:{y.sum()}/{len(y)}, X_test{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U CHI·∫æN D·ªäCH: GOLDEN FEATURES x MULTI-SEED...\n",
      "\n",
      "üå± --- SEED 42 (1/3) ---\n",
      "   Fold 1 Done.\n",
      "   Fold 2 Done.\n",
      "   Fold 3 Done.\n",
      "   Fold 4 Done.\n",
      "   Fold 5 Done.\n",
      "   üèÜ Best Seed 42: F1=0.6026 | Weights=(1, 3, 1)\n",
      "\n",
      "üå± --- SEED 2024 (2/3) ---\n",
      "   Fold 1 Done.\n",
      "   Fold 2 Done.\n",
      "   Fold 3 Done.\n",
      "   Fold 4 Done.\n",
      "   Fold 5 Done.\n",
      "   üèÜ Best Seed 2024: F1=0.5737 | Weights=(2, 1, 2)\n",
      "\n",
      "üå± --- SEED 123 (3/3) ---\n",
      "   Fold 1 Done.\n",
      "   Fold 2 Done.\n",
      "   Fold 3 Done.\n",
      "   Fold 4 Done.\n",
      "   Fold 5 Done.\n",
      "   üèÜ Best Seed 123: F1=0.6207 | Weights=(1, 1, 2)\n",
      "üéâ ƒê√É T·∫†O FILE: submission_golden_multiseed.csv\n",
      "üëâ File n√†y ch·ª©a s·ª©c m·∫°nh c·ªßa 54 Features tinh t√∫y x 3 Seeds x 3 Models!\n"
     ]
    }
   ],
   "source": [
    "X_selected = X\n",
    "\n",
    "golden_features = X_selected.columns.tolist() \n",
    "X_test_golden = df_test[golden_features]\n",
    "\n",
    "# Bi·∫øn t√≠ch l≈©y k·∫øt qu·∫£\n",
    "final_test_pred_accumulated = np.zeros(len(X_test_golden))\n",
    "\n",
    "print(f\" B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN...\")\n",
    "\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n --- SEED {seed} ({seed_idx + 1}/{len(SEEDS)}) ---\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Reset bi·∫øn OOF cho seed n√†y\n",
    "    oof_xgb = np.zeros(len(X_selected))\n",
    "    oof_lgbm = np.zeros(len(X_selected))\n",
    "    oof_cat = np.zeros(len(X_selected))\n",
    "    \n",
    "    seed_test_xgb = np.zeros(len(X_test_golden))\n",
    "    seed_test_lgbm = np.zeros(len(X_test_golden))\n",
    "    seed_test_cat = np.zeros(len(X_test_golden))\n",
    "    \n",
    "    # Update random_state\n",
    "    xgb_params['random_state'] = seed\n",
    "    lgbm_params['random_state'] = seed\n",
    "    cat_params['random_seed'] = seed\n",
    "\n",
    "    # Training Loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y)):\n",
    "        X_tr, X_val = X_selected.iloc[train_idx], X_selected.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # 1. XGBoost\n",
    "        model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "        model_xgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        oof_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n",
    "        seed_test_xgb += model_xgb.predict_proba(X_test_golden)[:, 1] / N_FOLDS\n",
    "\n",
    "        # 2. LightGBM\n",
    "        model_lgbm = lgbm.LGBMClassifier(**lgbm_params)\n",
    "        model_lgbm.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgbm.early_stopping(100, verbose=False)])\n",
    "        oof_lgbm[val_idx] = model_lgbm.predict_proba(X_val)[:, 1]\n",
    "        seed_test_lgbm += model_lgbm.predict_proba(X_test_golden)[:, 1] / N_FOLDS\n",
    "\n",
    "        # 3. CatBoost\n",
    "        train_pool = Pool(X_tr, y_tr)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "        model_cat = CatBoostClassifier(**cat_params)\n",
    "        model_cat.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "        oof_cat[val_idx] = model_cat.predict_proba(X_val)[:, 1]\n",
    "        seed_test_cat += model_cat.predict_proba(X_test_golden)[:, 1] / N_FOLDS\n",
    "        \n",
    "        print(f\"   Fold {fold+1} Done.\")\n",
    "\n",
    "    # T√¨m tr·ªçng s·ªë t·ªëi ∆∞u cho Seed n√†y\n",
    "    def get_best_f1(y_true, y_prob):\n",
    "        best_s, best_t = 0, 0\n",
    "        for th in np.arange(0.2, 0.8, 0.01):\n",
    "            s = f1_score(y_true, (y_prob >= th).astype(int))\n",
    "            if s > best_s: best_s, best_t = s, th\n",
    "        return best_s, best_t\n",
    "\n",
    "    # Grid search nh·ªè cho tr·ªçng s·ªë\n",
    "    weights_to_try = [\n",
    "        (1, 3, 1), # Tr·ªçng s·ªë chi·∫øn th·∫Øng c·ªßa b·∫°n v·ª´a r·ªìi\n",
    "        (1, 1, 1), (2, 2, 1), (1, 2, 1), (1, 1, 2), (2, 1, 2)\n",
    "    ]\n",
    "    \n",
    "    best_seed_f1 = 0\n",
    "    best_w = (1, 1, 1)\n",
    "    \n",
    "    for w1, w2, w3 in weights_to_try:\n",
    "        blend = (w1*oof_xgb + w2*oof_lgbm + w3*oof_cat) / (w1+w2+w3)\n",
    "        s, _ = get_best_f1(y, blend)\n",
    "        if s > best_seed_f1:\n",
    "            best_seed_f1 = s\n",
    "            best_w = (w1, w2, w3)\n",
    "            \n",
    "    print(f\"    Best Seed {seed}: F1={best_seed_f1:.4f} | Weights={best_w}\")\n",
    "    \n",
    "    # D·ª± b√°o Test c·ªßa Seed n√†y\n",
    "    w1, w2, w3 = best_w\n",
    "    seed_final_pred = (w1*seed_test_xgb + w2*seed_test_lgbm + w3*seed_test_cat) / (w1+w2+w3)\n",
    "    \n",
    "    final_test_pred_accumulated += seed_final_pred\n",
    "\n",
    "# --- K·∫æT QU·∫¢ CU·ªêI C√ôNG ---\n",
    "final_avg_prob = final_test_pred_accumulated / len(SEEDS)\n",
    "\n",
    "FINAL_TH = 0.50\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'object_id': df_test['object_id'],\n",
    "    'prediction': (final_avg_prob > FINAL_TH).astype(int)\n",
    "})\n",
    "\n",
    "filename = \"submission_golden_multiseed.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"üéâ ƒê√É T·∫†O FILE: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5850252a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫ÆT ƒê·∫¶U QU√Å TR√åNH HU·∫§N LUY·ªÜN....\n",
      "   -> ƒêang ch·∫°y Fold 1/5...\n",
      "   -> ƒêang ch·∫°y Fold 2/5...\n",
      "   -> ƒêang ch·∫°y Fold 3/5...\n",
      "   -> ƒêang ch·∫°y Fold 4/5...\n",
      "   -> ƒêang ch·∫°y Fold 5/5...\n",
      "K·∫øt qu·∫£ hu·∫•n luy·ªán:\n",
      "  - XGBoost   : F1 = 0.5871 (Thresh=0.39)\n",
      "  - LightGBM  : F1 = 0.5839 (Thresh=0.58)\n",
      "  - CatBoost  : F1 = 0.6012 (Thresh=0.45)\n"
     ]
    }
   ],
   "source": [
    "print(f\"B·∫ÆT ƒê·∫¶U QU√Å TR√åNH HU·∫§N LUY·ªÜN....\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_xgb = np.zeros(len(X))\n",
    "oof_lgbm = np.zeros(len(X))\n",
    "oof_cat = np.zeros(len(X))\n",
    "\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "test_lgbm = np.zeros(len(X_test))\n",
    "test_cat = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"   -> ƒêang ch·∫°y Fold {fold+1}/{N_FOLDS}...\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    oof_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n",
    "    test_xgb += model_xgb.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "\n",
    "    model_lgbm = lgbm.LGBMClassifier(**lgbm_params)\n",
    "    model_lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgbm.early_stopping(100, verbose=False)])\n",
    "    oof_lgbm[val_idx] = model_lgbm.predict_proba(X_val)[:, 1]\n",
    "    test_lgbm += model_lgbm.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "\n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    val_pool = Pool(X_val, y_val)\n",
    "    model_cat = CatBoostClassifier(**cat_params)\n",
    "    model_cat.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "    oof_cat[val_idx] = model_cat.predict_proba(val_pool)[:, 1]\n",
    "    test_cat += model_cat.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "def get_best_f1(y_true, y_prob):\n",
    "    best_s, best_t = 0, 0\n",
    "    for th in np.arange(0.1, 0.9, 0.01):\n",
    "        s = f1_score(y_true, (y_prob >= th).astype(int))\n",
    "        if s > best_s: best_s, best_t = s, th\n",
    "    return best_s, best_t\n",
    "\n",
    "print(\"K·∫øt qu·∫£ hu·∫•n luy·ªán:\")\n",
    "f1_xgb, th_xgb = get_best_f1(y, oof_xgb)\n",
    "print(f\"  - XGBoost   : F1 = {f1_xgb:.4f} (Thresh={th_xgb:.2f})\")\n",
    "f1_lgbm, th_lgbm = get_best_f1(y, oof_lgbm)\n",
    "print(f\"  - LightGBM  : F1 = {f1_lgbm:.4f} (Thresh={th_lgbm:.2f})\")\n",
    "f1_cat, th_cat = get_best_f1(y, oof_cat)\n",
    "print(f\"  - CatBoost  : F1 = {f1_cat:.4f} (Thresh={th_cat:.2f})\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89d24021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫æT QU·∫¢ ENSEMBLE:\n",
      "  - Tr·ªçng s·ªë: XGB=1, LGBM=3, CAT=1\n",
      "  - F1-Score: 0.6026\n",
      "  - Threshold: 0.52\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_w = (1, 1, 1)\n",
    "best_th = 0.5\n",
    "\n",
    "weights_to_try = [\n",
    "    (1, 1, 1), (2, 1, 1), (1, 2, 1), (1, 1, 2),\n",
    "    (2, 2, 1), (2, 1, 2), (1, 2, 2),\n",
    "    (3, 1, 1), (1, 3, 1), (1, 1, 3)\n",
    "]\n",
    "\n",
    "for w1, w2, w3 in weights_to_try:\n",
    "    blend = (w1*oof_xgb + w2*oof_lgbm + w3*oof_cat) / (w1+w2+w3)\n",
    "    s, th = get_best_f1(y, blend)\n",
    "    if s > best_f1:\n",
    "        best_f1 = s\n",
    "        best_w = (w1, w2, w3)\n",
    "        best_th = th\n",
    "\n",
    "print(f\"K·∫æT QU·∫¢ ENSEMBLE:\")\n",
    "print(f\"  - Tr·ªçng s·ªë: XGB={best_w[0]}, LGBM={best_w[1]}, CAT={best_w[2]}\")\n",
    "print(f\"  - F1-Score: {best_f1:.4f}\")\n",
    "print(f\"  - Threshold: {best_th:.2f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9694264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HO√ÄN T·∫§T! ƒê√£ t·∫°o file submission.csv.\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3 = best_w\n",
    "total_w = w1 + w2 + w3\n",
    "y_pred_blend = (w1 * test_xgb + w2 * test_lgbm + w3 * test_cat) / total_w\n",
    "submission = pd.DataFrame({\n",
    "    'object_id': df_test['object_id'],\n",
    "    'prediction': (y_pred_blend > best_th).astype(int)\n",
    "})\n",
    "\n",
    "sub_name = \"submission.csv\"\n",
    "submission.to_csv(sub_name, index=False)\n",
    "print(f\"\\n HO√ÄN T·∫§T! ƒê√£ t·∫°o file {sub_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
