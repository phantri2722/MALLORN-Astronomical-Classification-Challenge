{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14266730,"sourceType":"datasetVersion","datasetId":9104064}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install PyWavelets statsmodels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:09:36.299429Z","iopub.execute_input":"2025-12-24T08:09:36.299773Z","iopub.status.idle":"2025-12-24T08:09:42.496954Z","shell.execute_reply.started":"2025-12-24T08:09:36.299742Z","shell.execute_reply":"2025-12-24T08:09:42.495609Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (1.9.0)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\nRequirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from PyWavelets) (2.0.2)\nRequirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.15.3)\nRequirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport warnings\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import skew, kurtosis, linregress, median_abs_deviation\nfrom joblib import Parallel, delayed\n\n# Import các thư viện ML cổ điển\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import ConstantKernel, Matern\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\n# Import Boosting Models\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Tắt cảnh báo cho gọn\nwarnings.filterwarnings(\"ignore\")\n\n# ==============================================================================\n# 1. LIGHTCURVE PROCESSOR (GIỮ NGUYÊN - ĐÃ TỐI ƯU)\n# ==============================================================================\nclass LightCurveProcessor:\n    EXTINCTION_COEFFS = {'u': 4.239, 'g': 3.303, 'r': 2.285, 'i': 1.698, 'z': 1.263, 'y': 1.086}\n\n    def __init__(self, metadata_path):\n        self.metadata = pd.read_csv(metadata_path)\n        if 'object_id' in self.metadata.columns:\n            self.metadata.set_index('object_id', inplace=True)\n        \n    def correct_extinction(self, df):\n        if 'EBV' not in df.columns:\n            df = df.merge(self.metadata[['EBV']], left_on='object_id', right_index=True, how='left')\n        df['R_lambda'] = df['Filter'].map(self.EXTINCTION_COEFFS)\n        df['A_lambda'] = df['R_lambda'] * df['EBV']\n        correction_factor = np.power(10, 0.4 * df['A_lambda'])\n        df['Flux_corr'] = df['Flux'] * correction_factor\n        df['Flux_err_corr'] = df['Flux_err'] * correction_factor\n        df.dropna(subset=['Flux_corr'], inplace=True)\n        return df.drop(columns=['R_lambda', 'A_lambda', 'EBV'])\n\n    def load_and_process(self, lc_path):\n        raw_df = pd.read_csv(lc_path)\n        try:\n            return self.correct_extinction(raw_df)\n        except:\n            return raw_df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:24:40.495677Z","iopub.execute_input":"2025-12-24T12:24:40.496029Z","iopub.status.idle":"2025-12-24T12:24:40.508155Z","shell.execute_reply.started":"2025-12-24T12:24:40.496004Z","shell.execute_reply":"2025-12-24T12:24:40.507153Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# ==============================================================================\n# 2. PHYSICS FEATURE EXTRACTOR (GIỮ NGUYÊN BẢN V5 CỦA BẠN)\n# ==============================================================================\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import skew, kurtosis, linregress, median_abs_deviation, anderson\nfrom joblib import Parallel, delayed\n\nclass PhysicsFeatureExtractor:\n    def __init__(self):\n        self.bands = ['u', 'g', 'r', 'i', 'z', 'y']\n        # Cập nhật số lượng feature ước tính (Tăng thêm khoảng 10 feat/band)\n        self.n_feat_per_band = 58 \n        self.total_features = 0 \n\n    @staticmethod\n    def bazin(t, A, B, t0, t_fall, t_rise):\n        with np.errstate(over='ignore', invalid='ignore'):\n            exp_fall = np.exp(-(t - t0) / np.clip(t_fall, 1e-3, 500))\n            exp_rise = np.exp(-(t - t0) / np.clip(t_rise, 1e-3, 200))\n            val = A * (exp_fall / (1 + exp_rise)) + B\n        return np.nan_to_num(val)\n\n    def fit_bazin(self, times, fluxes):\n        if len(fluxes) <= 5: return [0]*6\n        try:\n            t_max_idx = np.argmax(fluxes)\n            t_max = times[t_max_idx]\n            f_max = fluxes[t_max_idx]\n            f_min = np.min(fluxes)\n            p0 = [f_max - f_min, f_min, t_max, 50, 20]\n            bounds = ([0, -np.inf, times.min()-50, 0.1, 0.1], [np.inf, np.inf, times.max()+50, 500, 200])\n            popt, _ = curve_fit(self.bazin, times, fluxes, p0=p0, bounds=bounds, maxfev=1000)\n            return list(popt) + [0]\n        except:\n            return [0]*6\n\n    def calculate_fft_features(self, times, fluxes):\n        \"\"\"Giữ nguyên FFT cũ nhưng thêm check độ dài\"\"\"\n        if len(fluxes) < 5: return [0]*6\n        try:\n            idx = np.argsort(times)\n            t_sorted, f_sorted = times[idx], fluxes[idx]\n            t_uniform = np.linspace(t_sorted[0], t_sorted[-1], len(t_sorted))\n            f_uniform = np.interp(t_uniform, t_sorted, f_sorted)\n            fft_vals = np.fft.rfft(f_uniform)\n            fft_freq = np.fft.rfftfreq(len(f_uniform))\n            power = np.abs(fft_vals[1:])\n            freqs = fft_freq[1:]\n            if len(power) == 0: return [0]*6\n            top_indices = np.argsort(power)[-3:][::-1]\n            res = []\n            for i in range(3):\n                if i < len(top_indices): res.extend([freqs[top_indices[i]], power[top_indices[i]]])\n                else: res.extend([0, 0])\n            return res\n        except:\n            return [0]*6\n\n    def calculate_stats(self, times, fluxes, errors, z_factor):\n        if len(fluxes) == 0: return [0] * (self.n_feat_per_band - 6) # Trừ 6 feat Bazin\n        \n        f_std = np.std(fluxes); f_mean = np.mean(fluxes)\n        f_max = np.max(fluxes); f_min = np.min(fluxes); f_median = np.median(fluxes)\n        \n        # --- GROUP 1: BASIC STATS (8) ---\n        res = [f_max, f_min, f_mean, f_std,\n               skew(fluxes) if len(fluxes) > 2 else 0,\n               kurtosis(fluxes) if len(fluxes) > 2 else 0,\n               (f_max - f_min) / 2, f_median]\n        \n        # --- GROUP 2: PERCENTILES & ROBUST RANGE (10) ---\n        ps = np.percentile(fluxes, [5, 10, 25, 30, 70, 75, 90, 95])\n        res.extend(ps)\n        res.append(ps[5] - ps[2]) # IQR (75-25)\n        res.append(ps[7] - ps[0]) # Robust Range (95-5) [NEW]\n        \n        # --- GROUP 3: RATIOS (5) ---\n        res.append(np.mean(fluxes**2))\n        res.append(np.mean(np.divide(fluxes, errors + 1e-6)))\n        res.append(np.sum(fluxes > f_mean) / len(fluxes))\n        res.append(np.sum(np.abs(fluxes - f_mean) > f_std) / len(fluxes))\n        res.append(len(fluxes))\n        \n        # --- GROUP 4: ADVANCED VARIABILITY (Stetson, Von Neumann) (4) ---\n        delta = np.divide(fluxes - f_mean, errors + 1e-9)\n        res.append(np.sum(np.abs(delta)) / len(fluxes) * np.sqrt(1.0/len(fluxes))) # Stetson K\n        \n        if len(fluxes) > 1:\n            eta = np.sum(np.diff(fluxes)**2) / (len(fluxes) - 1) / (f_std**2 + 1e-9)\n            res.append(eta) # Von Neumann\n            chi2 = np.sum(((fluxes - f_mean) / (errors + 1e-9))**2) / (len(fluxes) - 1)\n            res.append(chi2)\n            res.append(median_abs_deviation(fluxes, scale='normal')) # MAD\n        else: res.extend([0, 0, 0])\n\n        # --- GROUP 5: TIME DOMAIN DYNAMICS (AutoCorr, Derivative) (3) ---\n        if len(fluxes) > 2:\n            acf_1 = np.corrcoef(fluxes[:-1], fluxes[1:])[0, 1]\n            res.append(acf_1 if not np.isnan(acf_1) else 0)\n        else: res.append(0)\n\n        if len(fluxes) > 1:\n            diffs = np.diff(fluxes)\n            res.extend([np.mean(diffs), np.std(diffs)])\n        else: res.extend([0, 0])\n\n        # --- GROUP 6: LINEAR TRENDS (Slope) (2) ---\n        try:\n            slope, intercept, _, _, _ = linregress(times, fluxes)\n            res.extend([slope, intercept])\n        except: res.extend([0, 0])\n\n        # --- GROUP 7: [NEW] CUSUM & ANDERSON-DARLING (3) ---\n        # CUSUM: Cumulative Sum Range - Đo độ \"trôi\" của dữ liệu so với trung bình\n        # Giúp phân biệt biến thiên ngẫu nhiên (noise) vs biến thiên có cấu trúc\n        if len(fluxes) > 3:\n            cusum = np.cumsum(fluxes - f_mean)\n            res.append(np.max(cusum) - np.min(cusum)) # CUSUM Range\n            \n            # Anderson-Darling Test: Đo xem flux có tuân theo phân phối chuẩn không\n            # Black hole thường có \"heavy tail\" (đuôi nặng), không chuẩn\n            try:\n                ad_stat = anderson(fluxes, dist='norm').statistic\n                res.append(ad_stat)\n            except: res.append(0)\n        else:\n            res.extend([0, 0])\n\n        # Luminosity Proxy\n        res.append(f_max * z_factor)\n        \n        # --- GROUP 8: FFT (6) ---\n        res.extend(self.calculate_fft_features(times, fluxes))\n\n        # Padding (Tự động tính toán để khớp n_feat_per_band)\n        current_len = len(res)\n        padding_len = (self.n_feat_per_band - 6) - current_len\n        if padding_len > 0: res.extend([0] * padding_len)\n        \n        return res\n\n    def extract_features(self, df_object, meta_z=0, meta_ebv=0):\n        features = []; band_max_flux = {}; band_std = {}; band_slopes = {}\n        z_factor = meta_z**2 if meta_z > 0.001 else 1.0\n        \n        for band in self.bands:\n            band_data = df_object[df_object['Filter'] == band]\n            if len(band_data) < 3:\n                features.extend([0] * self.n_feat_per_band)\n            else:\n                times = band_data['Time'].values\n                fluxes = band_data['Flux_corr'].values\n                errors = band_data['Flux_err_corr'].values\n                \n                band_max_flux[band] = np.max(fluxes)\n                band_std[band] = np.std(fluxes)\n                \n                # Tính stats\n                stats = self.calculate_stats(times, fluxes, errors, z_factor)\n                \n                # Lưu slope để tính color evolution\n                # Slope nằm ở vị trí thứ 24 (index 23) trong stats (theo thứ tự add bên trên)\n                # Nhưng để an toàn ta tính lại nhanh\n                try:\n                    s, _, _, _, _ = linregress(times, fluxes)\n                    band_slopes[band] = s\n                except: band_slopes[band] = 0\n                \n                # Fit Bazin\n                bazin = self.fit_bazin(times, fluxes) if band in ['g', 'r', 'i'] else [0]*6\n                features.extend(stats + bazin)\n\n        # --- CROSS-BAND FEATURES (GLOBAL) ---\n        color_pairs = [('u', 'g'), ('g', 'r'), ('r', 'i'), ('i', 'z'), ('z', 'y')]\n        \n        for b1, b2 in color_pairs:\n            # 1. Flux Colors\n            val1 = band_max_flux.get(b1, 0); val2 = band_max_flux.get(b2, 0)\n            if val1 > 0 and val2 > 0:\n                c = -2.5 * np.log10(val1/val2)\n                features.extend([c, c * meta_z])\n            else: features.extend([0, 0])\n            \n            # 2. Amplitude & Std Ratios\n            amp1 = band_max_flux.get(b1, 1e-9); amp2 = band_max_flux.get(b2, 1e-9)\n            std1 = band_std.get(b1, 1e-9); std2 = band_std.get(b2, 1e-9)\n            features.append(amp1 / (amp2 + 1e-9))\n            features.append(std1 / (std2 + 1e-9))\n            \n            # 3. [NEW] Slope Differences (Color Evolution)\n            # Nếu u tăng nhanh hơn g -> Vật thể đang xanh hóa (becoming bluer)\n            s1 = band_slopes.get(b1, 0)\n            s2 = band_slopes.get(b2, 0)\n            features.append(s1 - s2)\n\n        # Metadata\n        features.extend([meta_z, meta_ebv])\n        \n        self.total_features = len(features)\n        return np.array(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:24:42.127967Z","iopub.execute_input":"2025-12-24T12:24:42.128329Z","iopub.status.idle":"2025-12-24T12:24:42.162178Z","shell.execute_reply.started":"2025-12-24T12:24:42.128302Z","shell.execute_reply":"2025-12-24T12:24:42.160920Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# ==============================================================================\n# 3. SMART FEATURE SELECTOR V3 (CHỌN LỌC TINH HOA)\n# ==============================================================================\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nclass SmartFeatureSelector:\n    def __init__(self, variance_thresh=0.0, correlation_thresh=0.99, max_features=300):\n        # max_features=300: Giữ lại 300 feature quan trọng nhất\n        self.var_thresh = VarianceThreshold(threshold=variance_thresh)\n        self.corr_thresh = correlation_thresh\n        self.max_features = max_features\n        self.drop_cols_corr = []\n        self.selector_model = None\n        \n    def fit(self, X, y):\n        # --- BƯỚC 1: Lọc Hằng Số (Variance) ---\n        print(\"-> [Selector] 1. Lọc Variance thấp...\")\n        self.var_thresh.fit(X)\n        X_v = self.var_thresh.transform(X)\n        print(f\"   Giữ lại {X_v.shape[1]} features sau bước Variance.\")\n        \n        # --- BƯỚC 2: Lọc Trùng Lặp (Correlation) ---\n        print(f\"-> [Selector] 2. Lọc Correlation cao (> {self.corr_thresh})...\")\n        # Dùng mẫu thử 10k dòng để tính correlation cho nhanh\n        if X_v.shape[0] > 10000:\n            idx = np.random.choice(X_v.shape[0], 10000, replace=False)\n            X_sample = X_v[idx]\n        else:\n            X_sample = X_v\n            \n        df_tmp = pd.DataFrame(X_sample)\n        corr_matrix = df_tmp.corr().abs()\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        \n        self.drop_cols_corr = [column for column in upper.columns if any(upper[column] > self.corr_thresh)]\n        print(f\"   Loại bỏ {len(self.drop_cols_corr)} features trùng lặp.\")\n        \n        # Chuẩn bị dữ liệu cho bước 3\n        # (Tạo mask để biết cột nào được giữ lại sau bước Correlation)\n        mask_corr = np.ones(X_v.shape[1], dtype=bool)\n        mask_corr[self.drop_cols_corr] = False\n        X_c = X_v[:, mask_corr]\n        \n        # --- BƯỚC 3: Lọc Quan Trọng (Feature Importance) ---\n        print(f\"-> [Selector] 3. Dùng LightGBM chọn Top {self.max_features} features...\")\n        \n        # LightGBM nhẹ để đánh giá\n        lgb = LGBMClassifier(n_estimators=100, learning_rate=0.05, \n                             random_state=42, n_jobs=-1, verbose=-1)\n        lgb.fit(X_c, y)\n        \n        # Chọn features có độ quan trọng cao nhất, giới hạn đúng số lượng max_features\n        self.selector_model = SelectFromModel(lgb, max_features=self.max_features, prefit=True)\n        \n        # In ra kết quả kiểm tra\n        selected_count = np.sum(self.selector_model.get_support())\n        print(f\"   => Đã chọn lọc được {selected_count} features tinh túy nhất từ {X_c.shape[1]}.\")\n            \n        return self\n\n    def transform(self, X):\n        # 1. Variance\n        X_v = self.var_thresh.transform(X)\n        \n        # 2. Correlation\n        mask_corr = np.ones(X_v.shape[1], dtype=bool)\n        mask_corr[self.drop_cols_corr] = False\n        X_c = X_v[:, mask_corr]\n        \n        # 3. Importance\n        X_final = self.selector_model.transform(X_c)\n        return X_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:24:45.735471Z","iopub.execute_input":"2025-12-24T12:24:45.736684Z","iopub.status.idle":"2025-12-24T12:24:45.748659Z","shell.execute_reply.started":"2025-12-24T12:24:45.736649Z","shell.execute_reply":"2025-12-24T12:24:45.747519Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# ==============================================================================\n# 4. HYBRID ENSEMBLE (GIỮ NGUYÊN - ĐÃ CẤU HÌNH TỐT)\n# ==============================================================================\nclass HybridEnsemble:\n    def __init__(self, scale_pos_weight=1.0):\n        # Tăng colsample_bytree và reg_alpha để chống nhiễu khi nhiều features\n        self.xgb = XGBClassifier(\n            n_estimators=600, learning_rate=0.03, max_depth=6,\n            scale_pos_weight=scale_pos_weight,\n            colsample_bytree=0.6, # Subsample features\n            reg_alpha=0.5,\n            eval_metric='logloss', use_label_encoder=False,\n            n_jobs=-1\n        )\n        self.lgbm = LGBMClassifier(\n            n_estimators=600, learning_rate=0.03,\n            scale_pos_weight=scale_pos_weight,\n            colsample_bytree=0.6,\n            reg_alpha=0.5,\n            verbose=-1\n        )\n        self.cat = CatBoostClassifier(\n            iterations=600, learning_rate=0.03, \n            scale_pos_weight=scale_pos_weight,\n            verbose=0, allow_writing_files=False,\n            task_type=\"CPU\" \n        )\n        self.meta = LogisticRegression()\n        self.scaler = StandardScaler()\n\n    def fit(self, X, y):\n        print(\"-> [Ensemble] Chuẩn hóa dữ liệu...\")\n        X_scaled = self.scaler.fit_transform(X)\n        \n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n        meta_features = np.zeros((len(y), 3))\n        \n        print(\"-> [Ensemble] Training Level 0 (CV Stacking)...\")\n        for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y)):\n            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            self.xgb.fit(X_train, y_train)\n            self.lgbm.fit(X_train, y_train)\n            self.cat.fit(X_train, y_train)\n            \n            meta_features[val_idx, 0] = self.xgb.predict_proba(X_val)[:, 1]\n            meta_features[val_idx, 1] = self.lgbm.predict_proba(X_val)[:, 1]\n            meta_features[val_idx, 2] = self.cat.predict_proba(X_val)[:, 1]\n            \n        print(\"-> [Ensemble] Retraining Level 0 on Full Data...\")\n        self.xgb.fit(X_scaled, y)\n        self.lgbm.fit(X_scaled, y)\n        self.cat.fit(X_scaled, y)\n        \n        print(\"-> [Ensemble] Training Level 1 (Meta Learner)...\")\n        self.meta.fit(meta_features, y)\n\n    def predict_proba(self, X):\n        X_scaled = self.scaler.transform(X)\n        p1 = self.xgb.predict_proba(X_scaled)[:, 1]\n        p2 = self.lgbm.predict_proba(X_scaled)[:, 1]\n        p3 = self.cat.predict_proba(X_scaled)[:, 1]\n        meta_features = np.column_stack([p1, p2, p3])\n        return self.meta.predict_proba(meta_features)[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:24:47.453616Z","iopub.execute_input":"2025-12-24T12:24:47.453936Z","iopub.status.idle":"2025-12-24T12:24:47.470208Z","shell.execute_reply.started":"2025-12-24T12:24:47.453907Z","shell.execute_reply":"2025-12-24T12:24:47.469207Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# ==============================================================================\n# 5. PROCESSING FUNCTION (BẢN FIX WARNING)\n# ==============================================================================\ndef process_single_object(obj_id, obj_data, target, n_augment, phys_extractor, meta_z, meta_ebv):\n    # --- [FIX 1] Import và Suppress Warning triệt để trong Worker ---\n    import warnings\n    from sklearn.exceptions import ConvergenceWarning\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    \n    try:\n        if obj_data.empty: return None\n        \n        # 1. Phys Features\n        try:\n            phys_feats = phys_extractor.extract_features(obj_data, meta_z, meta_ebv)\n        except: return None\n\n        # 2. Gaussian Process -> Raw Grid Features\n        GRID_POINTS = 50 \n        time_grid = np.linspace(-50, 150, GRID_POINTS) \n        \n        if 'Flux_corr' in obj_data.columns:\n            t_peak_global = obj_data.loc[obj_data['Flux_corr'].idxmax(), 'Time']\n        else:\n            t_peak_global = obj_data.loc[obj_data['flux'].idxmax(), 'Time']\n            \n        # --- [FIX 2] Nới rộng biên độ dưới (1e-10) để tránh ConvergenceWarning ---\n        # constant_value_bounds=(1e-10, 1000.0) cho phép nó tiến về 0 mà không bị warning\n        kernel = ConstantKernel(1.0, constant_value_bounds=(1e-10, 1000.0)) * \\\n                 Matern(length_scale=20, length_scale_bounds=(1, 500), nu=1.5)\n        \n        grid_feats_flat = []\n        \n        for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n            band_dat = obj_data[obj_data['Filter'] == band].copy()\n            \n            flux_col = 'Flux_corr' if 'Flux_corr' in band_dat.columns else 'flux'\n            err_col = 'Flux_err_corr' if 'Flux_err_corr' in band_dat.columns else 'flux_err'\n            \n            band_dat = band_dat.dropna(subset=['Time', flux_col, err_col])\n            if band_dat.duplicated(subset=['Time']).any():\n                band_dat = band_dat.groupby('Time', as_index=False).agg({flux_col:'mean', err_col:'mean'})\n                \n            if len(band_dat) < 3:\n                interp = np.zeros((n_augment + 1, GRID_POINTS))\n            else:\n                X = (band_dat['Time'].values - t_peak_global).reshape(-1, 1)\n                y = band_dat[flux_col].values\n                err = np.maximum(band_dat[err_col].values**2, 1e-6)\n                \n                try:\n                    # n_restarts_optimizer=0 để chạy nhanh hơn, tránh warning lặp lại\n                    gpr = GaussianProcessRegressor(kernel=kernel, alpha=err, \n                                                 optimizer='fmin_l_bfgs_b', \n                                                 n_restarts_optimizer=0, \n                                                 normalize_y=True)\n                    gpr.fit(X, y)\n                    \n                    mu = gpr.predict(time_grid.reshape(-1, 1))\n                    res = [mu]\n                    \n                    if n_augment > 0:\n                        samples = gpr.sample_y(time_grid.reshape(-1, 1), n_samples=n_augment)\n                        for k in range(n_augment): res.append(samples[:, k])\n                    \n                    interp = np.array(res)\n                except:\n                    interp = np.zeros((n_augment + 1, GRID_POINTS))\n            \n            grid_feats_flat.append(interp)\n\n        final_res = []\n        for i in range(n_augment + 1):\n            single_obj_grid = []\n            for b in range(6):\n                single_obj_grid.extend(grid_feats_flat[b][i])\n            \n            combined_feats = np.concatenate([phys_feats, np.array(single_obj_grid)])\n            \n            final_res.append({\n                'features': combined_feats,\n                'label': target,\n                'obj_id': obj_id\n            })\n            \n        return final_res\n    except Exception as e:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:24:54.804407Z","iopub.execute_input":"2025-12-24T12:24:54.804747Z","iopub.status.idle":"2025-12-24T12:24:54.822741Z","shell.execute_reply.started":"2025-12-24T12:24:54.804721Z","shell.execute_reply":"2025-12-24T12:24:54.821658Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# ==============================================================================\n# 6. MAIN PIPELINE\n# ==============================================================================\ndef main():\n    print(\"=== BẮT ĐẦU PIPELINE: FEATURE EXPLOSION & SMART SELECTION ===\")\n    \n    BASE_DIR = '/kaggle/input/data-for-black-hole'\n    if not os.path.exists(BASE_DIR):\n        BASE_DIR = '/content/drive/MyDrive/data-for-black-hole' # Fallback\n        \n    TRAIN_META = os.path.join(BASE_DIR, 'train_log.csv')\n    TEST_META = os.path.join(BASE_DIR, 'test_log.csv')\n    \n    train_proc = LightCurveProcessor(TRAIN_META)\n    test_proc = LightCurveProcessor(TEST_META) if os.path.exists(TEST_META) else None\n    \n    train_meta_dict = pd.read_csv(TRAIN_META).set_index('object_id')[['Z', 'EBV', 'target']].to_dict('index')\n    test_meta_dict = {}\n    if os.path.exists(TEST_META):\n        test_meta_dict = pd.read_csv(TEST_META).set_index('object_id')[['Z', 'EBV']].to_dict('index')\n\n    phys_extractor = PhysicsFeatureExtractor()\n    split_dirs = sorted([d for d in os.listdir(BASE_DIR) if d.startswith('split_')])\n    \n    ALL_X, ALL_Y = [], []\n    \n    # --- PROCESS TRAIN ---\n    print(\"\\n[1] XỬ LÝ DỮ LIỆU TRAIN & TẠO FEATURES KHỔNG LỒ...\")\n    for split in split_dirs:\n        path = os.path.join(BASE_DIR, split, 'train_full_lightcurves.csv')\n        if not os.path.exists(path): continue\n        print(f\"  >> Processing {split}...\")\n        \n        lc_df = train_proc.load_and_process(path)\n        col_map = {c: 'Time' for c in lc_df.columns if 'time' in c.lower()}\n        lc_df.rename(columns=col_map, inplace=True)\n        \n        tasks = []\n        for oid in lc_df['object_id'].unique():\n            if oid in train_meta_dict:\n                info = train_meta_dict[oid]\n                aug = 5 if info['target'] == 1 else 0\n                tasks.append((\n                    oid, lc_df[lc_df['object_id']==oid].copy(), \n                    info['target'], aug, \n                    phys_extractor, info['Z'], info['EBV']\n                ))\n        \n        results = Parallel(n_jobs=-1)(delayed(process_single_object)(*t) for t in tasks)\n        for r in results:\n            if r:\n                for item in r:\n                    ALL_X.append(item['features'])\n                    ALL_Y.append(item['label'])\n        \n        del lc_df, tasks; gc.collect()\n\n    X_arr = np.array(ALL_X)\n    y_arr = np.array(ALL_Y)\n    print(f\"-> Tổng số mẫu Train: {X_arr.shape[0]}\")\n    print(f\"-> Số features ban đầu (Explosion): {X_arr.shape[1]}\")\n    \n    # --- SMART FEATURE SELECTION ---\n    print(\"\\n[2] SMART FEATURE SELECTION (LỌC THÔNG MINH)...\")\n    selector = SmartFeatureSelector(variance_thresh=0.0, correlation_thresh=0.98)\n    # Fit vào tập train\n    selector.fit(X_arr)\n    X_clean = selector.transform(X_arr)\n    \n    print(f\"-> Số features sau khi lọc: {X_clean.shape[1]}\")\n    \n    # --- TRAINING ---\n    print(\"\\n[3] HUẤN LUYỆN MODEL ENSEMBLE (STACKING)...\")\n    pos = np.sum(y_arr==1); neg = np.sum(y_arr==0)\n    scale = neg / (pos + 1e-6)\n    print(f\"-> Scale Weight: {scale:.2f}\")\n    \n    ensemble = HybridEnsemble(scale_pos_weight=scale)\n    ensemble.fit(X_clean, y_arr)\n    \n    # --- TEST & SUBMIT ---\n    print(\"\\n[4] DỰ ĐOÁN TEST & SUBMISSION...\")\n    TEST_IDS, TEST_X_RAW = [], []\n    \n    for split in split_dirs:\n        path = os.path.join(BASE_DIR, split, 'test_full_lightcurves.csv')\n        if not os.path.exists(path): continue\n        print(f\"  >> Processing Test {split}...\")\n        \n        lc_df = test_proc.load_and_process(path)\n        col_map = {c: 'Time' for c in lc_df.columns if 'time' in c.lower()}\n        lc_df.rename(columns=col_map, inplace=True)\n        \n        tasks = []\n        for oid in lc_df['object_id'].unique():\n            z = test_meta_dict.get(oid, {}).get('Z', 0)\n            ebv = test_meta_dict.get(oid, {}).get('EBV', 0)\n            tasks.append((\n                oid, lc_df[lc_df['object_id']==oid].copy(), \n                0, 0, phys_extractor, z, ebv\n            ))\n            \n        results = Parallel(n_jobs=-1)(delayed(process_single_object)(*t) for t in tasks)\n        for r in results:\n            if r:\n                # Test thì không augment, r chỉ có 1 phần tử\n                TEST_X_RAW.append(r[0]['features'])\n                TEST_IDS.append(r[0]['obj_id'])\n                \n        del lc_df; gc.collect()\n        \n    if len(TEST_IDS) > 0:\n        X_test_arr = np.array(TEST_X_RAW)\n        # Transform tập test theo selector đã fit ở train\n        X_test_clean = selector.transform(X_test_arr)\n        \n        probs = ensemble.predict_proba(X_test_clean)\n        \n        # Đảm bảo output 0.0-1.0\n        probs = np.clip(probs, 0.0, 1.0)\n        \n        sub = pd.DataFrame({'object_id': TEST_IDS, 'target': probs})\n        \n        # Merge với sample submission để đảm bảo đủ ID\n        SAMPLE = os.path.join(BASE_DIR, 'sample_submission.csv')\n        if os.path.exists(SAMPLE):\n            sample_df = pd.read_csv(SAMPLE)\n            if 'target' in sample_df.columns: del sample_df['target']\n            sub = sample_df.merge(sub, on='object_id', how='left').fillna(0)\n            \n        sub.to_csv('/kaggle/working/submission.csv', index=False)\n        print(f\"✅ DONE! File saved with {len(sub)} rows.\")\n    else:\n        print(\"⚠️ No test data found.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:24:58.533029Z","iopub.execute_input":"2025-12-24T12:24:58.533377Z","iopub.status.idle":"2025-12-24T12:48:05.602518Z","shell.execute_reply.started":"2025-12-24T12:24:58.533351Z","shell.execute_reply":"2025-12-24T12:48:05.601108Z"}},"outputs":[{"name":"stdout","text":"=== BẮT ĐẦU PIPELINE: FEATURE EXPLOSION & SMART SELECTION ===\n\n[1] XỬ LÝ DỮ LIỆU TRAIN & TẠO FEATURES KHỔNG LỒ...\n  >> Processing split_01...\n  >> Processing split_02...\n  >> Processing split_03...\n  >> Processing split_04...\n  >> Processing split_05...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/scipy/optimize/_lsq/common.py:234: RuntimeWarning: overflow encountered in scalar divide\n  ratio = actual_reduction / predicted_reduction\n","output_type":"stream"},{"name":"stdout","text":"  >> Processing split_06...\n  >> Processing split_07...\n  >> Processing split_08...\n  >> Processing split_09...\n  >> Processing split_10...\n  >> Processing split_11...\n  >> Processing split_12...\n  >> Processing split_13...\n  >> Processing split_14...\n  >> Processing split_15...\n  >> Processing split_16...\n  >> Processing split_17...\n  >> Processing split_18...\n  >> Processing split_19...\n  >> Processing split_20...\n-> Tổng số mẫu Train: 3783\n-> Số features ban đầu (Explosion): 975\n\n[2] SMART FEATURE SELECTION (LỌC TINH HOA)...\n-> [Selector] 1. Lọc Variance thấp...\n   Giữ lại 888 features sau bước Variance.\n-> [Selector] 2. Lọc Correlation cao (> 0.995)...\n   Loại bỏ 292 features trùng lặp.\n-> [Selector] 3. Dùng LightGBM chọn Top 300 features...\n   => Đã chọn lọc được 159 features tinh túy nhất từ 596.\n-> Số features sau khi lọc: 159\n\n[3] HUẤN LUYỆN MODEL ENSEMBLE (STACKING)...\n-> Scale Weight: 3.26\n-> [Ensemble] Chuẩn hóa dữ liệu...\n-> [Ensemble] Training Level 0 (CV Stacking)...\n-> [Ensemble] Retraining Level 0 on Full Data...\n-> [Ensemble] Training Level 1 (Meta Learner)...\n\n[4] DỰ ĐOÁN TEST & SUBMISSION...\n  >> Processing Test split_01...\n  >> Processing Test split_02...\n  >> Processing Test split_03...\n  >> Processing Test split_04...\n  >> Processing Test split_05...\n  >> Processing Test split_06...\n  >> Processing Test split_07...\n  >> Processing Test split_08...\n  >> Processing Test split_09...\n  >> Processing Test split_10...\n  >> Processing Test split_11...\n  >> Processing Test split_12...\n  >> Processing Test split_13...\n  >> Processing Test split_14...\n  >> Processing Test split_15...\n  >> Processing Test split_16...\n  >> Processing Test split_17...\n  >> Processing Test split_18...\n  >> Processing Test split_19...\n  >> Processing Test split_20...\n✅ DONE! File saved with 7135 rows.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Đọc file kết quả (đảm bảo đường dẫn đúng với nơi file được sinh ra)\n# Thường là /kaggle/working/submission.csv\nfile_path = '/kaggle/working/submission.csv' \ndf = pd.read_csv(file_path)\n\n# 2. Áp dụng ngưỡng 0.001 trên cột chứa xác suất (cột 'target')\n# Nếu target > 0.001 -> gán là 1, ngược lại là 0\ndf['prediction'] = (df['target'] > 0.1).astype(int)\n\n# 3. Chỉ giữ lại 2 cột cần thiết: object_id và prediction\nfinal_submission = df[['object_id', 'prediction']]\n\n# 4. Lưu lại file mới hoặc ghi đè file cũ để nộp\nfinal_submission.to_csv('submission_lightgbm++_01.csv', index=False)\n\nprint(\"✅ Đã xử lý xong!\")\nprint(\"Số lượng TDE (nhãn 1) dự đoán được:\", final_submission['prediction'].sum())\nprint(final_submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:56:36.929194Z","iopub.execute_input":"2025-12-24T12:56:36.929579Z","iopub.status.idle":"2025-12-24T12:56:36.958989Z","shell.execute_reply.started":"2025-12-24T12:56:36.929511Z","shell.execute_reply":"2025-12-24T12:56:36.957799Z"}},"outputs":[{"name":"stdout","text":"✅ Đã xử lý xong!\nSố lượng TDE (nhãn 1) dự đoán được: 374\n                      object_id  prediction\n0      Eluwaith_Mithrim_nothrim           0\n1            Eru_heledir_archam           0\n2             Gonhir_anann_fuin           0\n3  Gwathuirim_haradrim_tegilbor           0\n4              achas_minai_maen           0\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Đọc file kết quả chứa xác suất\ndf = pd.read_csv('submission.csv')\n\n# 2. Áp dụng ngưỡng 0.001 để tạo cột 'prediction' (0 hoặc 1)\ndf['prediction'] = (df['target'] > 0.0008).astype(int)\n\n# 3. Chỉ giữ lại 2 cột cần thiết: 'object_id' và 'prediction'\nfinal_df = df[['object_id', 'prediction']]\n\n# 4. Lưu file final\nfinal_df.to_csv('submission_final_0008.csv', index=False)\n\nprint(\"Đã lưu file submission_final.csv\")\nprint(final_df.head())\nprint(\"\\nThống kê số lượng:\")\nprint(final_df['prediction'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T13:19:52.630559Z","iopub.execute_input":"2025-12-23T13:19:52.631125Z","iopub.status.idle":"2025-12-23T13:19:52.666404Z","shell.execute_reply.started":"2025-12-23T13:19:52.631093Z","shell.execute_reply":"2025-12-23T13:19:52.665452Z"}},"outputs":[{"name":"stdout","text":"Đã lưu file submission_final.csv\n                      object_id  prediction\n0      Eluwaith_Mithrim_nothrim           0\n1            Eru_heledir_archam           0\n2             Gonhir_anann_fuin           0\n3  Gwathuirim_haradrim_tegilbor           0\n4              achas_minai_maen           0\n\nThống kê số lượng:\nprediction\n0    6706\n1     429\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":46}]}