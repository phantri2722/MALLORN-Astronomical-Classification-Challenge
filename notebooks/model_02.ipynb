{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60412a9d",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ffd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "BASE_DIR = \"/MALLORN-Astronomical-Classification-Challenge/data/raw\"\n",
    "\n",
    "# Hệ số bước sóng cho các filter (dùng để khử Extinction)\n",
    "LAMBDA = {\n",
    "    'u': 4.81,\n",
    "    'g': 3.64,\n",
    "    'r': 2.70,\n",
    "    'i': 2.06,\n",
    "    'z': 1.58,\n",
    "    'y': 1.30\n",
    "}\n",
    "\n",
    "def process_data(mode='train'):\n",
    "    print(f\"--- Processing {mode} data ---\")\n",
    "    log_file = os.path.join(BASE_DIR, f'{mode}_log.csv')\n",
    "    df_log = pd.read_csv(log_file)\n",
    "    \n",
    "    # Chuyển object_id sang string để map chính xác\n",
    "    df_log['object_id'] = df_log['object_id'].astype(str)\n",
    "\n",
    "    # 1. TẠO DICTIONARY CHO METADATA\n",
    "    # Chúng ta cần cả EBV (để sửa Flux) và Z (để sửa Time & tính Luminosity sau này)\n",
    "    # Lưu ý: Test set cũng có Z (photometric), dùng tốt!\n",
    "    meta_map = df_log.set_index('object_id')[['EBV', 'Z']].to_dict('index')\n",
    "\n",
    "    all_chunks = []\n",
    "    unique_splits = df_log['split'].unique()\n",
    "\n",
    "    dtypes = {\n",
    "        'object_id': str,\n",
    "        'Time (MJD)': 'float32',\n",
    "        'Flux': 'float32',\n",
    "        'Flux_err': 'float32'\n",
    "    }\n",
    "\n",
    "    for split_name in tqdm(unique_splits, desc=f\"Loading {mode} splits\"):\n",
    "        file_path = os.path.join(BASE_DIR, split_name, f'{mode}_full_lightcurves.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Not found {file_path}\")\n",
    "            continue\n",
    "\n",
    "        df_chunk = pd.read_csv(file_path, dtype=dtypes)\n",
    "\n",
    "        # 2. MAP METADATA VÀO CHUNK\n",
    "        # Map EBV và Z vào từng dòng dữ liệu\n",
    "        # Sử dụng map trực tiếp từ dict sẽ nhanh hơn merge\n",
    "        # Lưu ý: Cần xử lý trường hợp key không tìm thấy (mặc dù hiếm)\n",
    "        df_chunk['EBV'] = df_chunk['object_id'].map(lambda x: meta_map.get(x, {}).get('EBV', 0.0))\n",
    "        df_chunk['Z'] = df_chunk['object_id'].map(lambda x: meta_map.get(x, {}).get('Z', 0.0))\n",
    "        \n",
    "        # 3. APPLY DE-EXTINCTION\n",
    "        # Tính hệ số R_lambda\n",
    "        df_chunk['R_lambda'] = df_chunk['Filter'].map(LAMBDA)\n",
    "        \n",
    "        # Công thức: Flux_corr = Flux * 10^(0.4 * EBV * R)\n",
    "        correction_factor = 10 ** (0.4 * df_chunk['EBV'] * df_chunk['R_lambda'])\n",
    "        \n",
    "        df_chunk['Flux_corrected'] = df_chunk['Flux'] * correction_factor\n",
    "        df_chunk['Flux_err_corrected'] = df_chunk['Flux_err'] * correction_factor\n",
    "        \n",
    "        # Ép kiểu float32 để tiết kiệm RAM\n",
    "        cols_float = ['Flux_corrected', 'Flux_err_corrected', 'Z', 'EBV']\n",
    "        for col in cols_float:\n",
    "            df_chunk[col] = df_chunk[col].astype('float32')\n",
    "\n",
    "        # 4. CHỌN CỘT CẦN THIẾT\n",
    "        # Giữ lại Z để dùng cho cell tạo Tensor tiếp theo\n",
    "        cols_to_keep = [\n",
    "            'object_id', \n",
    "            'Time (MJD)', \n",
    "            'Flux_corrected', \n",
    "            'Flux_err_corrected', \n",
    "            'Filter', \n",
    "            'Z' # <--- QUAN TRỌNG\n",
    "        ]\n",
    "        all_chunks.append(df_chunk[cols_to_keep])\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"No data loaded!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    full_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "    if mode == 'train':\n",
    "        full_df = full_df.merge(df_log[['object_id', 'target']], on='object_id', how='left')\n",
    "        full_df['target'] = full_df['target'].astype('int8')\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "# --- RUN ---\n",
    "train_df_clean = process_data(mode='train')\n",
    "test_df_clean = process_data(mode='test')\n",
    "\n",
    "# Tạo thư mục nếu chưa có\n",
    "os.makedirs(\"/MALLORN-Astronomical-Classification-Challenge/data/processed/\", exist_ok=True)\n",
    "\n",
    "print(\"Saving to parquet...\")\n",
    "train_df_clean.to_parquet(\"/MALLORN-Astronomical-Classification-Challenge/data/processed/train_lightcurves_clean.parquet\")\n",
    "test_df_clean.to_parquet(\"/MALLORN-Astronomical-Classification-Challenge/data/processed/test_lightcurves_clean.parquet\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710674f2",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_FILE = '/MALLORN-Astronomical-Classification-Challenge/data/processed/train_lightcurves_clean.parquet'\n",
    "OUTPUT_FILE = '/MALLORN-Astronomical-Classification-Challenge/data/processed/train_tensor.pt'\n",
    "MAX_SEQ_LEN = 200\n",
    "FILTER_MAP = {'u': 0, 'g': 1, 'r': 2, 'i': 3, 'z': 4, 'y': 5, 'Y': 5}\n",
    "\n",
    "def preprocess(input_path, output_path, is_train=True):\n",
    "    print(f\"Loading data from {input_path}...\")\n",
    "    df = pd.read_parquet(input_path)\n",
    "    \n",
    "    # 1. Map Filter ID\n",
    "    df['filter_id'] = df['Filter'].map(FILTER_MAP).fillna(0).astype('int8')\n",
    "\n",
    "    # 2. Sort để đảm bảo tính tuần tự thời gian\n",
    "    df = df.sort_values(by=['object_id', 'Time (MJD)'])\n",
    "\n",
    "    print(\"Grouping data...\")\n",
    "    # Thêm 'Z' vào aggregate\n",
    "    agg_df = df.groupby('object_id').agg({\n",
    "        'Flux_corrected': list,\n",
    "        'Flux_err_corrected': list,\n",
    "        'Time (MJD)': list,\n",
    "        'filter_id': list,\n",
    "        'Z': 'first',      # Z là hằng số với mỗi object\n",
    "        'target': 'first'  # Target (nếu có)\n",
    "    }).reset_index()\n",
    "    \n",
    "    num_samples = len(agg_df)\n",
    "    \n",
    "    # --- KHỞI TẠO TENSOR VỚI KÍCH THƯỚC MỚI ---\n",
    "    # Input Dim = 4: [Flux_Arcsinh, Err_Log, Time_Rest, Redshift]\n",
    "    X_num = np.zeros((num_samples, MAX_SEQ_LEN, 4), dtype=np.float32) \n",
    "    X_cat = np.zeros((num_samples, MAX_SEQ_LEN), dtype=np.int64)\n",
    "    X_mask = np.zeros((num_samples, MAX_SEQ_LEN), dtype=np.float32)\n",
    "    y = np.zeros(num_samples, dtype=np.int64) # LongTensor cho CrossEntropy\n",
    "\n",
    "    ids = agg_df['object_id'].values\n",
    "\n",
    "    print(\"Building tensors with Physics transformations...\")\n",
    "    iterator = zip(\n",
    "        agg_df['Flux_corrected'],\n",
    "        agg_df['Flux_err_corrected'],\n",
    "        agg_df['Time (MJD)'],\n",
    "        agg_df['filter_id'],\n",
    "        agg_df['Z'],\n",
    "        agg_df['target']\n",
    "    )\n",
    "\n",
    "    for i, (flux, flux_err, times, filters, z, target) in enumerate(tqdm(iterator, total=num_samples)):\n",
    "        # Convert list -> numpy array\n",
    "        flux = np.array(flux, dtype=np.float32)\n",
    "        flux_err = np.array(flux_err, dtype=np.float32)\n",
    "        times = np.array(times, dtype=np.float32)\n",
    "        filters = np.array(filters, dtype=np.int64)\n",
    "        z_val = float(z)\n",
    "\n",
    "        # --- PHYSICS TRANSFORMATION (QUAN TRỌNG) ---\n",
    "        \n",
    "        # 1. Flux Scaling: Dùng Arcsinh thay vì StandardScaler\n",
    "        # Giữ được tính chất Flux âm của AGN và nén được Flux dương cực lớn của TDE\n",
    "        flux_trans = np.arcsinh(flux)\n",
    "        \n",
    "        # 2. Error Scaling: Logarit\n",
    "        # Cộng thêm epsilon nhỏ (1e-5) để tránh log(0) nếu có error=0\n",
    "        err_trans = np.log1p(flux_err)\n",
    "\n",
    "        # 3. Time Dilation Correction\n",
    "        # Tính khoảng thời gian trôi qua\n",
    "        time_delta_obs = times - times[0]\n",
    "        # Chuyển về Rest-frame (Thời gian thực tại nguồn)\n",
    "        time_delta_rest = time_delta_obs / (1 + z_val)\n",
    "        # Normalize: Chia cho 100 ngày để giá trị về khoảng nhỏ (0-1, 0-2...) cho model dễ học\n",
    "        time_trans = time_delta_rest / 100.0\n",
    "\n",
    "        seq_len = len(flux)\n",
    "        limit = min(seq_len, MAX_SEQ_LEN)\n",
    "\n",
    "        # --- FILL TENSOR ---\n",
    "        # Feature 0: Flux (Arcsinh)\n",
    "        X_num[i, :limit, 0] = flux_trans[:limit]\n",
    "        \n",
    "        # Feature 1: Error (Log)\n",
    "        X_num[i, :limit, 1] = err_trans[:limit]\n",
    "        \n",
    "        # Feature 2: Time (Rest-frame & Normalized)\n",
    "        # Giúp model hiểu đúng tốc độ biến thiên của sự kiện\n",
    "        X_num[i, :limit, 2] = time_trans[:limit]\n",
    "        \n",
    "        # Feature 3: Redshift (Z)\n",
    "        # Cung cấp ngữ cảnh khoảng cách (Distance Context)\n",
    "        X_num[i, :limit, 3] = z_val\n",
    "\n",
    "        X_cat[i, :limit] = filters[:limit]\n",
    "        X_mask[i, :limit] = 1.0 # Mask padding\n",
    "\n",
    "        # Xử lý target (có thể là NaN trong tập test, ta để tạm 0 hoặc -1)\n",
    "        if pd.notna(target):\n",
    "            y[i] = int(target)\n",
    "        else:\n",
    "            y[i] = 0\n",
    "\n",
    "    # Save\n",
    "    data_dict = {\n",
    "        'x_num': torch.tensor(X_num),\n",
    "        'x_cat': torch.tensor(X_cat),\n",
    "        'x_mask': torch.tensor(X_mask),\n",
    "        'y': torch.tensor(y),\n",
    "        'ids': ids\n",
    "    }\n",
    "    \n",
    "    # Không cần lưu scaler nữa vì ta dùng hàm toán học cố định (Arcsinh/Log)\n",
    "    \n",
    "    print(f\"Saving to {output_path}\")\n",
    "    torch.save(data_dict, output_path)\n",
    "\n",
    "# --- RUN ---\n",
    "preprocess(\n",
    "    input_path=INPUT_FILE,\n",
    "    output_path=OUTPUT_FILE,\n",
    "    is_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b912913",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# --- 1. DATASET (Giữ nguyên) ---\n",
    "class MallornDataset(Dataset):\n",
    "    def __init__(self, tensors_path, mode='train'):\n",
    "        data = torch.load(tensors_path, weights_only=False)\n",
    "        self.x_num = data['x_num'] \n",
    "        self.x_cat = data['x_cat'] \n",
    "        self.mask = data['x_mask'] \n",
    "        self.y = data['y']         \n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x_num': self.x_num[idx],\n",
    "            'x_cat': self.x_cat[idx],\n",
    "            'mask': self.mask[idx],\n",
    "            'y': self.y[idx]\n",
    "        }\n",
    "\n",
    "# --- 2. TIME ENCODING (Giữ nguyên) ---\n",
    "class TimeEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(TimeEncoding, self).__init__()\n",
    "        self.linear = nn.Linear(1, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, time_values):\n",
    "        t = time_values.unsqueeze(-1) \n",
    "        return self.activation(self.linear(t))\n",
    "\n",
    "# --- 3. TRANSFORMER MODEL (ĐÃ FIX LỖI DIMENSION) ---\n",
    "class TDETransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim=4,\n",
    "                 num_filters=6, \n",
    "                 d_model=128, \n",
    "                 nhead=4, \n",
    "                 num_layers=3, \n",
    "                 dim_feedforward=256, \n",
    "                 dropout=0.1,\n",
    "                 num_classes=1):\n",
    "        super(TDETransformer, self).__init__()\n",
    "        \n",
    "        self.content_dim = input_dim - 1 \n",
    "        self.content_projection = nn.Linear(self.content_dim, d_model)\n",
    "        \n",
    "        self.filter_embedding = nn.Embedding(num_filters, d_model)\n",
    "        self.time_encoding = TimeEncoding(d_model)\n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat, mask):\n",
    "        # 1. Prepare Input\n",
    "        time_data = x_num[:, :, 2] \n",
    "        flux_err = x_num[:, :, :2] \n",
    "        z_val = x_num[:, :, 3].unsqueeze(-1)\n",
    "        content_input = torch.cat([flux_err, z_val], dim=-1) \n",
    "\n",
    "        # 2. Embedding\n",
    "        x_content = self.content_projection(content_input)\n",
    "        x_filter = self.filter_embedding(x_cat)\n",
    "        x_time = self.time_encoding(time_data)\n",
    "        \n",
    "        x = x_content + x_filter + x_time\n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        # 3. Transformer\n",
    "        padding_mask = (mask == 0) \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # --- THAY ĐỔI LỚN: MAX POOLING ---\n",
    "        # Để Max Pooling không bắt phải giá trị padding (thường là 0),\n",
    "        # ta gán giá trị padding thành âm vô cùng (-1e9)\n",
    "        mask_expanded = mask.unsqueeze(-1) # [Batch, Seq, 1]\n",
    "        \n",
    "        # Fill những chỗ mask=0 bằng số cực nhỏ\n",
    "        x_masked_for_max = x.masked_fill(mask_expanded == 0, -1e9)\n",
    "        \n",
    "        # Lấy giá trị lớn nhất dọc theo chiều Sequence (dim=1)\n",
    "        # Giúp bắt được đỉnh sáng của TDE bất kể nó nằm ở đâu\n",
    "        max_embeddings = x_masked_for_max.max(dim=1)[0] # [Batch, d_model]\n",
    "        \n",
    "        # 4. Classify\n",
    "        out = self.dropout(max_embeddings)\n",
    "        logits = self.classifier(out) \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41ebae",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# --- CONFIG (Đã cập nhật cho khớp với Model mới) ---\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'batch_size': 32,      # Tăng lên 32 để batch norm hoạt động tốt hơn\n",
    "    'virtual_batch_size': 128,\n",
    "    'epochs': 15,          # Tăng nhẹ số epoch\n",
    "    'learning_rate': 2e-4, # LR chuẩn cho Transformer\n",
    "    'weight_decay': 1e-3,\n",
    "    'd_model': 128,        # Khớp với Cell 3\n",
    "    'nhead': 4,\n",
    "    'num_layers': 3,\n",
    "    'dim_feedforward': 256,\n",
    "    'dropout': 0.1,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'num_workers': 0       # Set = 0 để tránh lỗi Windows/RAM\n",
    "}\n",
    "\n",
    "accumulation_steps = CONFIG['virtual_batch_size'] // CONFIG['batch_size']\n",
    "\n",
    "# --- 1. DATA LOADER (Với Soft Pos Weight) ---\n",
    "def prepare_loaders(dataset_path):\n",
    "    print(f\"Loading dataset from {dataset_path}...\")\n",
    "    full_dataset = MallornDataset(dataset_path)\n",
    "    \n",
    "    # Lấy targets để stratify\n",
    "    targets = full_dataset.y.numpy()\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(full_dataset)),\n",
    "        test_size=0.2,\n",
    "        random_state=CONFIG['seed'],\n",
    "        stratify=targets\n",
    "    )\n",
    "\n",
    "    train_data = Subset(full_dataset, train_idx)\n",
    "    val_data = Subset(full_dataset, val_idx)\n",
    "\n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    # Loader\n",
    "    train_loader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_data, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # --- SOFT POS_WEIGHT CALCULATION ---\n",
    "    train_targets = targets[train_idx]\n",
    "    num_pos = train_targets.sum()\n",
    "    num_neg = len(train_idx) - num_pos\n",
    "    \n",
    "    ratio = num_neg / max(num_pos, 1)\n",
    "    soft_weight = np.sqrt(ratio) # Căn bậc 2 để giảm bớt sự cực đoan\n",
    "    \n",
    "    # Kẹp giá trị trong khoảng hợp lý [3.0, 10.0]\n",
    "    soft_weight = np.clip(soft_weight, 3.0, 10.0)\n",
    "    \n",
    "    print(f\"Original Ratio: {ratio:.2f} -> Soft Weight Used: {soft_weight:.2f}\")\n",
    "    \n",
    "    pos_weight = torch.tensor([soft_weight], dtype=torch.float32).to(CONFIG['device'])\n",
    "\n",
    "    return train_loader, val_loader, pos_weight\n",
    "\n",
    "# --- 2. TRAIN FUNCTION (Sửa lỗi thiếu backward) ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, accumulation_steps, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "\n",
    "    for i, batch in enumerate(pbar):\n",
    "        x_num = batch['x_num'].to(device, non_blocking=True)\n",
    "        x_cat = batch['x_cat'].to(device, non_blocking=True)\n",
    "        mask = batch['mask'].to(device, non_blocking=True)\n",
    "        y = batch['y'].float().unsqueeze(1).to(device, non_blocking=True)\n",
    "\n",
    "        # Robust clamping\n",
    "        x_num = torch.nan_to_num(x_num, nan=0.0)\n",
    "        x_num = torch.clamp(x_num, min=-10.0, max=10.0) \n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(x_num, x_cat, mask)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        # --- FIX: THÊM BACKWARD ---\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_val = loss.item() * accumulation_steps\n",
    "        if not math.isnan(loss_val):\n",
    "             total_loss += loss_val\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss_val:.4f}\"})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- 3. VALIDATE FUNCTION (Dò tìm Threshold tối ưu) ---\n",
    "def validate_find_threshold(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Dùng list để gom batch array nhanh hơn extend từng phần tử\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validating', leave=False):\n",
    "            x_num = batch['x_num'].to(device, non_blocking=True)\n",
    "            x_cat = batch['x_cat'].to(device, non_blocking=True)\n",
    "            mask = batch['mask'].to(device, non_blocking=True)\n",
    "            y = batch['y'].float().unsqueeze(1).to(device, non_blocking=True)\n",
    "\n",
    "            x_num = torch.nan_to_num(x_num, nan=0.0)\n",
    "            x_num = torch.clamp(x_num, min=-10.0, max=10.0)\n",
    "\n",
    "            outputs = model(x_num, x_cat, mask)\n",
    "            \n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "\n",
    "    # Nối mảng lớn một lần\n",
    "    if not all_preds: return float('inf'), 0, 0, 0, 0.5\n",
    "    \n",
    "    all_probs = np.concatenate(all_preds).ravel()\n",
    "    all_targets = np.concatenate(all_targets).ravel()\n",
    "\n",
    "    # --- THRESHOLD SEARCH ---\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.001\n",
    "    prec, rec = 0, 0\n",
    "    \n",
    "    # Quét từ 0.1 đến 0.9\n",
    "    for t in np.arange(0.1, 0.9, 0.05):\n",
    "        preds = (all_probs > t).astype(int)\n",
    "        if preds.sum() == 0: continue\n",
    "        \n",
    "        score = f1_score(all_targets, preds)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_thresh = t\n",
    "            prec = precision_score(all_targets, preds, zero_division=0)\n",
    "            rec = recall_score(all_targets, preds)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    # Debug: In ra xác suất trung bình để biết model tự tin cỡ nào\n",
    "    print(f\"   [Debug] Avg Prob: {all_probs.mean():.4f} | Max Prob: {all_probs.max():.4f}\")\n",
    "    \n",
    "    return avg_loss, best_f1, prec, rec, best_thresh\n",
    "\n",
    "# --- 4. MAIN RUN ---\n",
    "def run_training_final():\n",
    "    # Load Data\n",
    "    train_loader, val_loader, _ = prepare_loaders('/MALLORN-Astronomical-Classification-Challenge/data/processed/train_tensor.pt')\n",
    "    \n",
    "    # --- THAY ĐỔI QUYẾT ĐỊNH ---\n",
    "    # Gán cứng pos_weight = 2.0 (Thay vì để code tự tính ra 5 hay 10)\n",
    "    # Ý nghĩa: Chỉ ưu tiên TDE gấp đôi nhiễu thôi, không được spam.\n",
    "    print(\"!!! FORCE OVERRIDE: Setting pos_weight = 5.0 !!!\")\n",
    "    pos_weight = torch.tensor([5.0]).to(CONFIG['device'])\n",
    "    \n",
    "    # Init Model (input_dim=4)\n",
    "    model = TDETransformer(\n",
    "        input_dim=4, \n",
    "        d_model=CONFIG['d_model'],\n",
    "        nhead=CONFIG['nhead'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    # Loss & Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    \n",
    "    scheduler = OneCycleLR(optimizer, max_lr=CONFIG['learning_rate'], \n",
    "                          steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    best_f1_global = 0.0\n",
    "    print(\"\\n--- START TRAINING (Low Weight Strategy) ---\")\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, CONFIG['device'], accumulation_steps, scaler)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_f1, val_prec, val_rec, thresh = validate_find_threshold(model, val_loader, criterion, CONFIG['device'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | \"\n",
    "              f\"F1: {val_f1:.4f} (at {thresh:.2f}) [P: {val_prec:.2f}, R: {val_rec:.2f}]\")\n",
    "        \n",
    "        if val_f1 > best_f1_global:\n",
    "            best_f1_global = val_f1\n",
    "            torch.save(model.state_dict(), '/MALLORN-Astronomical-Classification-Challenge/models/best_physics_model.pth')\n",
    "            print(\"--> Model Saved!\")\n",
    "\n",
    "# --- EXECUTE ---\n",
    "if __name__ == '__main__':\n",
    "    run_training_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea764e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_inference():\n",
    "    print(\"\\n--- START INFERENCE ---\")\n",
    "    \n",
    "    # 1. Load Test Data\n",
    "    # Đảm bảo bạn đã chạy preprocess cho test set để tạo ra file này\n",
    "    test_file = '/MALLORN-Astronomical-Classification-Challenge/data/processed/test_tensor.pt'\n",
    "    if not os.path.exists(test_file):\n",
    "        # Nếu chưa có thì chạy preprocess cho test\n",
    "        print(\"Generating test tensors...\")\n",
    "        preprocess(\n",
    "            input_path='/MALLORN-Astronomical-Classification-Challenge/data/processed/test_lightcurves_clean.parquet',\n",
    "            output_path=test_file,\n",
    "            is_train=False\n",
    "        )\n",
    "        \n",
    "    test_dataset = MallornDataset(test_file, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CONFIG['batch_size'] * 2, # Batch lớn hơn cho nhanh (không cần backward)\n",
    "        shuffle=False, \n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 2. Load Best Model\n",
    "    model = TDETransformer(\n",
    "        input_dim=4, # Khớp với config train\n",
    "        d_model=CONFIG['d_model'],\n",
    "        nhead=CONFIG['nhead'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=0.0 # Không dropout khi dự đoán\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    model_path = '/MALLORN-Astronomical-Classification-Challenge/models/best_physics_model.pth'\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model not found at {model_path}. Did you train successfully?\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CONFIG['device']))\n",
    "    model.eval()\n",
    "    \n",
    "    # 3. Predict\n",
    "    all_preds = []\n",
    "    all_ids = test_dataset.x_num # ID nằm trong data gốc lúc save, nhưng dataset class chưa trả về ID\n",
    "    # Fix lại cách lấy ID từ file saved\n",
    "    data = torch.load(test_file, weights_only=False)\n",
    "    object_ids = data['ids']\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    idx_counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            x_num = batch['x_num'].to(CONFIG['device'])\n",
    "            x_cat = batch['x_cat'].to(CONFIG['device'])\n",
    "            mask = batch['mask'].to(CONFIG['device'])\n",
    "            \n",
    "            x_num = torch.nan_to_num(x_num, nan=0.0)\n",
    "            x_num = torch.clamp(x_num, min=-10.0, max=10.0)\n",
    "            \n",
    "            logits = model(x_num, x_cat, mask)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "            \n",
    "            # Map predictions to Object IDs\n",
    "            batch_size = len(probs)\n",
    "            current_ids = object_ids[idx_counter : idx_counter + batch_size]\n",
    "            \n",
    "            for obj_id, prob in zip(current_ids, probs):\n",
    "                predictions[obj_id] = prob\n",
    "                \n",
    "            idx_counter += batch_size\n",
    "\n",
    "    # 4. Create Submission DataFrame\n",
    "    # Load sample submission để đảm bảo đúng format (nếu có)\n",
    "    # Hoặc tạo mới từ predictions\n",
    "    sub_df = pd.DataFrame({\n",
    "        'object_id': list(predictions.keys()),\n",
    "        'target': list(predictions.values())\n",
    "    })\n",
    "    \n",
    "    # Quan trọng: Dùng Threshold tối ưu đã tìm được lúc Train\n",
    "    # Nếu bạn nhớ threshold (ví dụ 0.25), bạn có thể convert sang 0/1 ở đây\n",
    "    # Nhưng thường Kaggle yêu cầu xác suất (float). Nếu yêu cầu nhãn 0/1:\n",
    "    # sub_df['target'] = (sub_df['target'] > BEST_THRESHOLD).astype(int)\n",
    "    \n",
    "    # Lưu file\n",
    "    output_csv = 'submission.csv'\n",
    "    sub_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Submission saved to {output_csv}. Rows: {len(sub_df)}\")\n",
    "    print(sub_df.head())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
